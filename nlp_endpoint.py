"""
 ENDPOINTY NLP (Natural Language Processing)
=============================================

API endpoints dla przetwarzania jzyka naturalnego z u偶yciem spaCy.
Udostpnia funkcje analizy tekstu, ekstrakcji encji, analizy sentymentu itp.

Autor: Zaawansowany System Kognitywny ahui69
Data: 19 pa藕dziernika 2025
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import asyncio
import time

from core.nlp_processor import get_nlp_processor
from core.config import AUTH_TOKEN
from core.helpers import log_info, log_error

# Utw贸rz router
router = APIRouter(prefix="/api/nlp", tags=["nlp"])

# Modele Pydantic dla request/response
class TextAnalysisRequest(BaseModel):
    text: str = Field(..., description="Tekst do analizy", min_length=1, max_length=10000)
    use_cache: bool = Field(True, description="Czy u偶ywa cache'a dla wynik贸w")

class BatchAnalysisRequest(BaseModel):
    texts: List[str] = Field(..., description="Lista tekst贸w do analizy", min_items=1, max_items=50)
    batch_size: int = Field(10, description="Rozmiar wsadu", ge=1, le=20)

class TopicExtractionRequest(BaseModel):
    texts: List[str] = Field(..., description="Lista tekst贸w do ekstrakcji temat贸w", min_items=1, max_items=100)
    num_topics: int = Field(5, description="Liczba temat贸w do ekstrakcji", ge=1, le=20)

class NLPAnalysisResponse(BaseModel):
    text: str
    language: str
    tokens: List[Dict[str, Any]]
    entities: List[Dict[str, Any]]
    sentiment: Dict[str, float]
    key_phrases: List[str]
    pos_tags: List[Dict[str, str]]
    dependencies: List[Dict[str, Any]]
    readability_score: float
    processing_time: float

class BatchAnalysisResponse(BaseModel):
    results: List[NLPAnalysisResponse]
    total_texts: int
    total_processing_time: float

class TopicExtractionResponse(BaseModel):
    topics: List[str]
    num_texts_processed: int
    processing_time: float

class NLPStatsResponse(BaseModel):
    total_analyses: int
    cache_hits: int
    cache_size: int
    avg_processing_time: float
    language_distribution: Dict[str, int]
    models_loaded: List[str]

# Funkcje pomocnicze
def verify_auth_token(token: str):
    """Weryfikuje token autoryzacyjny"""
    if token != AUTH_TOKEN:
        raise HTTPException(status_code=401, detail="Nieprawidowy token autoryzacyjny")

# Endpointy
@router.post("/analyze", response_model=NLPAnalysisResponse)
async def analyze_text(
    request: TextAnalysisRequest,
    authorization: str = None
) -> NLPAnalysisResponse:
    """
    Przeprowadza kompleksow analiz NLP pojedynczego tekstu

    - **text**: Tekst do analizy (1-10000 znak贸w)
    - **use_cache**: Czy u偶ywa cache'a (domylnie True)
    """
    if authorization:
        token = authorization.replace("Bearer ", "")
        verify_auth_token(token)

    try:
        processor = get_nlp_processor()
        result = await processor.analyze_text(request.text, request.use_cache)

        log_info(f"[NLP_ENDPOINT] Przeanalizowano tekst: {len(request.text)} znak贸w, jzyk: {result.language}")
        return result

    except Exception as e:
        log_error(f"[NLP_ENDPOINT] Bd analizy tekstu: {e}")
        raise HTTPException(status_code=500, detail=f"Bd analizy tekstu: {str(e)}")

@router.post("/batch-analyze", response_model=BatchAnalysisResponse)
async def batch_analyze_texts(
    request: BatchAnalysisRequest,
    background_tasks: BackgroundTasks,
    authorization: str = None
) -> BatchAnalysisResponse:
    """
    Przeprowadza analiz NLP dla wielu tekst贸w wsadowo

    - **texts**: Lista tekst贸w (1-50 tekst贸w)
    - **batch_size**: Rozmiar wsadu (1-20)
    """
    if authorization:
        token = authorization.replace("Bearer ", "")
        verify_auth_token(token)

    start_time = time.time()

    try:
        processor = get_nlp_processor()
        results = await processor.batch_analyze(request.texts, request.batch_size)

        total_time = time.time() - start_time

        log_info(f"[NLP_ENDPOINT] Przeanalizowano wsadowo {len(request.texts)} tekst贸w w {total_time:.2f}s")
        return BatchAnalysisResponse(
            results=results,
            total_texts=len(request.texts),
            total_processing_time=total_time
        )

    except Exception as e:
        log_error(f"[NLP_ENDPOINT] Bd analizy wsadowej: {e}")
        raise HTTPException(status_code=500, detail=f"Bd analizy wsadowej: {str(e)}")

@router.post("/extract-topics", response_model=TopicExtractionResponse)
async def extract_topics(
    request: TopicExtractionRequest,
    authorization: str = None
) -> TopicExtractionResponse:
    """
    Ekstrahuje tematy z kolekcji tekst贸w

    - **texts**: Lista tekst贸w (1-100 tekst贸w)
    - **num_topics**: Liczba temat贸w do ekstrakcji (1-20)
    """
    if authorization:
        token = authorization.replace("Bearer ", "")
        verify_auth_token(token)

    start_time = time.time()

    try:
        processor = get_nlp_processor()
        topics = await processor.extract_topics(request.texts, request.num_topics)

        processing_time = time.time() - start_time

        log_info(f"[NLP_ENDPOINT] Wyekstrahowano {len(topics)} temat贸w z {len(request.texts)} tekst贸w")
        return TopicExtractionResponse(
            topics=topics,
            num_texts_processed=len(request.texts),
            processing_time=processing_time
        )

    except Exception as e:
        log_error(f"[NLP_ENDPOINT] Bd ekstrakcji temat贸w: {e}")
        raise HTTPException(status_code=500, detail=f"Bd ekstrakcji temat贸w: {str(e)}")

@router.get("/stats", response_model=NLPStatsResponse)
async def get_nlp_stats(authorization: str = None) -> NLPStatsResponse:
    """
    Zwraca statystyki procesora NLP

    - Statystyki u偶ycia, wydajnoci i modeli jzykowych
    """
    if authorization:
        token = authorization.replace("Bearer ", "")
        verify_auth_token(token)

    try:
        processor = get_nlp_processor()
        stats = processor.get_stats()

        return NLPStatsResponse(**stats)

    except Exception as e:
        log_error(f"[NLP_ENDPOINT] Bd pobierania statystyk: {e}")
        raise HTTPException(status_code=500, detail=f"Bd pobierania statystyk: {str(e)}")

@router.post("/entities")
async def extract_entities(
    request: TextAnalysisRequest,
    authorization: str = None
) -> Dict[str, Any]:
    """
    Ekstrahuje encje nazwane z tekstu

    - **text**: Tekst do analizy
    - **use_cache**: Czy u偶ywa cache'a
    """
    if authorization:
        token = authorization.replace("Bearer ", "")
        verify_auth_token(token)

    try:
        processor = get_nlp_processor()
        result = await processor.analyze_text(request.text, request.use_cache)

        return {
            "text": request.text,
            "language": result.language,
            "entities": result.entities,
            "entity_count": len(result.entities),
            "processing_time": result.processing_time
        }

    except Exception as e:
        log_error(f"[NLP_ENDPOINT] Bd ekstrakcji encji: {e}")
        raise HTTPException(status_code=500, detail=f"Bd ekstrakcji encji: {str(e)}")

@router.post("/sentiment")
async def analyze_sentiment(
    request: TextAnalysisRequest,
    authorization: str = None
) -> Dict[str, Any]:
    """
    Analizuje sentyment tekstu

    - **text**: Tekst do analizy
    - **use_cache**: Czy u偶ywa cache'a
    """
    if authorization:
        token = authorization.replace("Bearer ", "")
        verify_auth_token(token)

    try:
        processor = get_nlp_processor()
        result = await processor.analyze_text(request.text, request.use_cache)

        return {
            "text": request.text,
            "language": result.language,
            "sentiment": result.sentiment,
            "processing_time": result.processing_time
        }

    except Exception as e:
        log_error(f"[NLP_ENDPOINT] Bd analizy sentymentu: {e}")
        raise HTTPException(status_code=500, detail=f"Bd analizy sentymentu: {str(e)}")

@router.post("/key-phrases")
async def extract_key_phrases(
    request: TextAnalysisRequest,
    authorization: str = None
) -> Dict[str, Any]:
    """
    Ekstrahuje frazy kluczowe z tekstu

    - **text**: Tekst do analizy
    - **use_cache**: Czy u偶ywa cache'a
    """
    if authorization:
        token = authorization.replace("Bearer ", "")
        verify_auth_token(token)

    try:
        processor = get_nlp_processor()
        result = await processor.analyze_text(request.text, request.use_cache)

        return {
            "text": request.text,
            "language": result.language,
            "key_phrases": result.key_phrases,
            "phrase_count": len(result.key_phrases),
            "processing_time": result.processing_time
        }

    except Exception as e:
        log_error(f"[NLP_ENDPOINT] Bd ekstrakcji fraz kluczowych: {e}")
        raise HTTPException(status_code=500, detail=f"Bd ekstrakcji fraz kluczowych: {str(e)}")

@router.post("/readability")
async def calculate_readability(
    request: TextAnalysisRequest,
    authorization: str = None
) -> Dict[str, Any]:
    """
    Oblicza ocen czytelnoci tekstu

    - **text**: Tekst do analizy
    - **use_cache**: Czy u偶ywa cache'a
    """
    if authorization:
        token = authorization.replace("Bearer ", "")
        verify_auth_token(token)

    try:
        processor = get_nlp_processor()
        result = await processor.analyze_text(request.text, request.use_cache)

        return {
            "text": request.text,
            "language": result.language,
            "readability_score": result.readability_score,
            "processing_time": result.processing_time
        }

    except Exception as e:
        log_error(f"[NLP_ENDPOINT] Bd obliczania czytelnoci: {e}")
        raise HTTPException(status_code=500, detail=f"Bd obliczania czytelnoci: {str(e)}")

# Log inicjalizacji
log_info("[NLP_ENDPOINT] Zarejestrowano endpointy NLP: /api/nlp/*")