"""
ğŸ§  MODUÅ PRZETWARZANIA JÄ˜ZYKA NATURALNEGO (NLP)
==============================================

Zaawansowany procesor NLP oparty na spaCy dla analizy tekstu,
ekstrakcji encji, analizy sentymentu i innych zadaÅ„ jÄ™zykowych.

Autor: Zaawansowany System Kognitywny ahui69
Data: 19 paÅºdziernika 2025
"""

import asyncio
import json
import re
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import time

import spacy
from spacy.lang.pl import Polish
from spacy.lang.en import English

from .config import *
from .helpers import log_info, log_error, log_warning

@dataclass
class NLPAnalysisResult:
    """Wynik analizy NLP tekstu"""
    text: str
    language: str
    tokens: List[Dict[str, Any]]
    entities: List[Dict[str, Any]]
    sentiment: Dict[str, float]
    key_phrases: List[str]
    pos_tags: List[Dict[str, str]]
    dependencies: List[Dict[str, Any]]
    readability_score: float
    processing_time: float

@dataclass
class Entity:
    """Encja nazwana"""
    text: str
    label: str
    start: int
    end: int
    confidence: float

@dataclass
class SentimentAnalysis:
    """Analiza sentymentu"""
    polarity: float  # -1 do 1
    subjectivity: float  # 0 do 1
    emotions: Dict[str, float]
    intensity: float

class NLPProcessor:
    """
    ğŸ§  Zaawansowany Procesor NLP oparty na spaCy

    FunkcjonalnoÅ›ci:
    - Tokenizacja i lematyzacja
    - Ekstrakcja encji nazwanych (NER)
    - Analiza czÄ™Å›ci mowy (POS tagging)
    - Analiza zaleÅ¼noÅ›ci skÅ‚adniowych
    - Ekstrakcja fraz kluczowych
    - Analiza sentymentu
    - Wykrywanie jÄ™zyka
    - Ocena czytelnoÅ›ci tekstu
    """

    def __init__(self):
        self.models = {}
        self._load_models()

        # Cache dla wynikÃ³w analizy
        self.analysis_cache = {}
        self.cache_max_size = 1000

        # Statystyki przetwarzania
        self.stats = {
            "total_analyses": 0,
            "cache_hits": 0,
            "processing_times": [],
            "language_distribution": defaultdict(int)
        }

        log_info("[NLP_PROCESSOR] Procesor NLP zainicjalizowany")

    def _load_models(self):
        """Åaduje modele jÄ™zykowe spaCy"""
        try:
            # Model polski
            self.models['pl'] = spacy.load('pl_core_news_sm')
            log_info("[NLP_PROCESSOR] ZaÅ‚adowano model polski: pl_core_news_sm")

            # Model angielski jako fallback
            self.models['en'] = spacy.load('en_core_web_sm')
            log_info("[NLP_PROCESSOR] ZaÅ‚adowano model angielski: en_core_web_sm")

        except OSError as e:
            log_error(f"[NLP_PROCESSOR] BÅ‚Ä…d Å‚adowania modeli spaCy: {e}")
            # Fallback do podstawowych modeli
            self.models['pl'] = Polish()
            self.models['en'] = English()
            log_warning("[NLP_PROCESSOR] UÅ¼ywam podstawowych modeli jÄ™zykowych")

    def _detect_language(self, text: str) -> str:
        """Wykrywa jÄ™zyk tekstu"""
        # Prosta heurystyka jÄ™zykowa
        polish_chars = set('Ä…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼')
        english_chars = set('abcdefghijklmnopqrstuvwxyz')

        text_lower = text.lower()
        polish_count = sum(1 for char in text_lower if char in polish_chars)
        english_count = sum(1 for char in text_lower if char in english_chars)

        # JeÅ›li wiÄ™cej polskich znakÃ³w - polski
        if polish_count > english_count * 0.1:
            return 'pl'

        # DomyÅ›lnie angielski
        return 'en'

    async def analyze_text(self, text: str, use_cache: bool = True) -> NLPAnalysisResult:
        """
        Przeprowadza kompleksowÄ… analizÄ™ NLP tekstu

        Args:
            text: Tekst do analizy
            use_cache: Czy uÅ¼ywaÄ‡ cache'a

        Returns:
            NLPAnalysisResult: Kompletny wynik analizy
        """
        start_time = time.time()

        # SprawdÅº cache
        cache_key = hash(text)
        if use_cache and cache_key in self.analysis_cache:
            self.stats["cache_hits"] += 1
            cached_result = self.analysis_cache[cache_key]
            cached_result.processing_time = time.time() - start_time
            return cached_result

        try:
            # Wykryj jÄ™zyk
            language = self._detect_language(text)
            self.stats["language_distribution"][language] += 1

            # Pobierz odpowiedni model
            nlp_model = self.models.get(language, self.models.get('en'))

            # PrzetwÃ³rz tekst
            doc = nlp_model(text)

            # PrzeprowadÅº analizy
            tokens = self._extract_tokens(doc)
            entities = self._extract_entities(doc)
            sentiment = self._analyze_sentiment(text, language)
            key_phrases = self._extract_key_phrases(doc)
            pos_tags = self._extract_pos_tags(doc)
            dependencies = self._extract_dependencies(doc)
            readability = self._calculate_readability(text, language)

            # UtwÃ³rz wynik
            result = NLPAnalysisResult(
                text=text,
                language=language,
                tokens=tokens,
                entities=entities,
                sentiment=sentiment,
                key_phrases=key_phrases,
                pos_tags=pos_tags,
                dependencies=dependencies,
                readability_score=readability,
                processing_time=time.time() - start_time
            )

            # Zapisz w cache
            if use_cache:
                if len(self.analysis_cache) >= self.cache_max_size:
                    # UsuÅ„ najstarszy wpis
                    oldest_key = next(iter(self.analysis_cache))
                    del self.analysis_cache[oldest_key]
                self.analysis_cache[cache_key] = result

            self.stats["total_analyses"] += 1
            processing_time = time.time() - start_time
            self.stats["processing_times"].append(processing_time)

            log_info(f"[NLP_PROCESSOR] Przeanalizowano tekst ({language}): {len(text)} znakÃ³w w {processing_time:.3f}s")
            return result

        except Exception as e:
            log_error(f"[NLP_PROCESSOR] BÅ‚Ä…d analizy tekstu: {e}")
            # ZwrÃ³Ä‡ minimalny wynik bÅ‚Ä™du
            return NLPAnalysisResult(
                text=text,
                language="unknown",
                tokens=[],
                entities=[],
                sentiment={"polarity": 0.0, "subjectivity": 0.0, "emotions": {}, "intensity": 0.0},
                key_phrases=[],
                pos_tags=[],
                dependencies=[],
                readability_score=0.0,
                processing_time=time.time() - start_time
            )

    def _extract_tokens(self, doc) -> List[Dict[str, Any]]:
        """Ekstrahuje tokeny z dokumentu spaCy"""
        tokens = []
        for token in doc:
            token_info = {
                "text": token.text,
                "lemma": token.lemma_,
                "pos": token.pos_,
                "tag": token.tag_,
                "dep": token.dep_,
                "is_alpha": token.is_alpha,
                "is_stop": token.is_stop,
                "is_punct": token.is_punct,
                "like_num": token.like_num,
                "like_email": token.like_email,
                "like_url": token.like_url
            }
            tokens.append(token_info)
        return tokens

    def _extract_entities(self, doc) -> List[Dict[str, Any]]:
        """Ekstrahuje encje nazwane"""
        entities = []
        for ent in doc.ents:
            entity_info = {
                "text": ent.text,
                "label": ent.label_,
                "start": ent.start_char,
                "end": ent.end_char,
                "confidence": getattr(ent, '_.confidence', 1.0)  # JeÅ›li dostÄ™pne
            }
            entities.append(entity_info)
        return entities

    def _analyze_sentiment(self, text: str, language: str) -> Dict[str, float]:
        """Analizuje sentyment tekstu"""
        # Prosta analiza sentymentu oparta na sÅ‚owach kluczowych
        # W produkcji moÅ¼na uÅ¼yÄ‡ dedykowanych modeli sentymentu

        positive_words = {
            'pl': ['dobry', 'Å›wietny', 'fantastyczny', 'wspaniaÅ‚y', 'doskonale', 'Å›wietnie', 'super', 'genialny', 'znakomity', 'Å›wietny'],
            'en': ['good', 'great', 'excellent', 'amazing', 'wonderful', 'perfect', 'awesome', 'brilliant', 'outstanding', 'superb']
        }

        negative_words = {
            'pl': ['zÅ‚y', 'okropny', 'straszny', 'fatalny', 'beznadziejny', 'przeraÅ¼ajÄ…cy', 'okropnie', 'fatalnie', 'Åºle', 'gorszy'],
            'en': ['bad', 'terrible', 'awful', 'horrible', 'dreadful', 'atrocious', 'abominable', 'appalling', 'dire', 'execrable']
        }

        emotion_words = {
            'radoÅ›Ä‡': ['radoÅ›Ä‡', 'szczÄ™Å›cie', 'wesoÅ‚oÅ›Ä‡', 'euforia', 'zachwyt'],
            'smutek': ['smutek', 'Å¼al', 'przygnÄ™bienie', 'depresja', 'rozpacz'],
            'gniew': ['gniew', 'zÅ‚oÅ›Ä‡', 'wÅ›ciekÅ‚oÅ›Ä‡', 'furia', 'irytacja'],
            'strach': ['strach', 'lÄ™k', 'przeraÅ¼enie', 'panika', 'obawa'],
            'zaskoczenie': ['zaskoczenie', 'zdziwienie', 'zdumienie', 'osÅ‚upienie'],
            'wstrÄ™t': ['wstrÄ™t', 'obrzydzenie', 'odraza', 'niesmak']
        }

        text_lower = text.lower()
        words = re.findall(r'\b\w+\b', text_lower)

        # Policz sÅ‚owa pozytywne i negatywne
        pos_count = sum(1 for word in words if word in positive_words.get(language, positive_words['en']))
        neg_count = sum(1 for word in words if word in negative_words.get(language, negative_words['en']))

        total_sentiment_words = pos_count + neg_count
        if total_sentiment_words == 0:
            polarity = 0.0
        else:
            polarity = (pos_count - neg_count) / total_sentiment_words

        # Analiza emocji
        emotions = {}
        for emotion, keywords in emotion_words.items():
            count = sum(1 for word in words if word in keywords)
            emotions[emotion] = count / len(words) if words else 0.0

        # IntensywnoÅ›Ä‡ (proporcja sÅ‚Ã³w sentymentu)
        intensity = total_sentiment_words / len(words) if words else 0.0

        # SubiektywnoÅ›Ä‡ (heurystyka)
        subjectivity = min(1.0, intensity * 2)

        return {
            "polarity": polarity,
            "subjectivity": subjectivity,
            "emotions": emotions,
            "intensity": intensity
        }

    def _extract_key_phrases(self, doc) -> List[str]:
        """Ekstrahuje frazy kluczowe"""
        # Prosta ekstrakcja oparta na rzeczownikach i przymiotnikach
        key_phrases = []

        # ZnajdÅº rzeczowniki
        nouns = [token.lemma_ for token in doc if token.pos_ in ['NOUN', 'PROPN']]

        # ZnajdÅº przymiotniki
        adjectives = [token.lemma_ for token in doc if token.pos_ == 'ADJ']

        # PoÅ‚Ä…cz w frazy (rzeczownik + przymiotnik)
        for noun in nouns[:10]:  # Max 10 fraz
            # ZnajdÅº przymiotniki blisko rzeczownika
            nearby_adjectives = []
            for token in doc:
                if token.lemma_ == noun and token.pos_ in ['NOUN', 'PROPN']:
                    # SprawdÅº tokeny w okolicy
                    start = max(0, token.i - 3)
                    end = min(len(doc), token.i + 4)
                    for nearby_token in doc[start:end]:
                        if nearby_token.pos_ == 'ADJ' and nearby_token.lemma_ not in nearby_adjectives:
                            nearby_adjectives.append(nearby_token.lemma_)

            if nearby_adjectives:
                phrase = f"{nearby_adjectives[0]} {noun}"
                key_phrases.append(phrase)
            else:
                key_phrases.append(noun)

        return list(set(key_phrases))  # UsuÅ„ duplikaty

    def _extract_pos_tags(self, doc) -> List[Dict[str, str]]:
        """Ekstrahuje tagi czÄ™Å›ci mowy"""
        pos_tags = []
        for token in doc:
            pos_tags.append({
                "token": token.text,
                "pos": token.pos_,
                "tag": token.tag_,
                "dep": token.dep_
            })
        return pos_tags

    def _extract_dependencies(self, doc) -> List[Dict[str, Any]]:
        """Ekstrahuje relacje zaleÅ¼noÅ›ci skÅ‚adniowych"""
        dependencies = []
        for token in doc:
            dep_info = {
                "token": token.text,
                "head": token.head.text if token.head != token else "ROOT",
                "dep": token.dep_,
                "children": [child.text for child in token.children]
            }
            dependencies.append(dep_info)
        return dependencies

    def _calculate_readability(self, text: str, language: str) -> float:
        """Oblicza ocenÄ™ czytelnoÅ›ci tekstu"""
        # Implementacja uproszczonej formuÅ‚y czytelnoÅ›ci (np. Flesch-Kincaid)
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        words = re.findall(r'\b\w+\b', text)
        syllables = sum(self._count_syllables(word, language) for word in words)

        if not sentences or not words:
            return 0.0

        avg_sentence_length = len(words) / len(sentences)
        avg_syllables_per_word = syllables / len(words)

        # PolskojÄ™zyczna adaptacja Flesch-Kincaid
        if language == 'pl':
            readability = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_syllables_per_word)
        else:
            # Standardowa formuÅ‚a dla angielskiego
            readability = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_syllables_per_word)

        # Normalizuj do 0-1 (wyÅ¼sze = bardziej czytelne)
        return max(0.0, min(1.0, readability / 100.0))

    def _count_syllables(self, word: str, language: str) -> int:
        """Liczy sylaby w sÅ‚owie"""
        word = word.lower()

        if language == 'pl':
            # Prosta heurystyka dla polskiego
            vowels = 'aeiouyÄ…Ä™Ã³'
            syllables = 0
            prev_vowel = False

            for char in word:
                if char in vowels:
                    if not prev_vowel:
                        syllables += 1
                    prev_vowel = True
                else:
                    prev_vowel = False

            return max(1, syllables)
        else:
            # Dla angielskiego - uproszczona wersja
            vowels = 'aeiouy'
            syllables = 0
            prev_vowel = False

            for char in word:
                if char in vowels:
                    if not prev_vowel:
                        syllables += 1
                    prev_vowel = True
                else:
                    prev_vowel = False

            # Korekty
            if word.endswith('e'):
                syllables -= 1
            if word.endswith('le') and len(word) > 2 and word[-3] not in vowels:
                syllables += 1

            return max(1, syllables)

    async def extract_topics(self, texts: List[str], num_topics: int = 5) -> List[str]:
        """
        Ekstrahuje tematy z kolekcji tekstÃ³w (prosta implementacja)

        Args:
            texts: Lista tekstÃ³w do analizy
            num_topics: Liczba tematÃ³w do ekstrakcji

        Returns:
            List[str]: Lista gÅ‚Ã³wnych tematÃ³w
        """
        try:
            all_tokens = []

            for text in texts:
                analysis = await self.analyze_text(text, use_cache=True)
                # Zbierz rzeczowniki i przymiotniki
                content_words = [
                    token['lemma'] for token in analysis.tokens
                    if not token['is_stop'] and token['pos'] in ['NOUN', 'PROPN', 'ADJ']
                ]
                all_tokens.extend(content_words)

            # Policz najczÄ™Å›ciej wystÄ™pujÄ…ce sÅ‚owa
            word_counts = Counter(all_tokens)
            top_words = [word for word, count in word_counts.most_common(num_topics * 2)]

            # Grupuj podobne sÅ‚owa (prosta heurystyka)
            topics = []
            used_words = set()

            for word in top_words:
                if word not in used_words:
                    # ZnajdÅº podobne sÅ‚owa
                    similar = [w for w in top_words if self._words_similar(word, w) and w not in used_words]
                    if similar:
                        topics.append(" ".join(similar[:3]))  # Max 3 sÅ‚owa w temacie
                        used_words.update(similar)
                    else:
                        topics.append(word)
                        used_words.add(word)

                if len(topics) >= num_topics:
                    break

            return topics[:num_topics]

        except Exception as e:
            log_error(f"[NLP_PROCESSOR] BÅ‚Ä…d ekstrakcji tematÃ³w: {e}")
            return []

    def _words_similar(self, word1: str, word2: str) -> bool:
        """Sprawdza czy sÅ‚owa sÄ… podobne (prosta heurystyka)"""
        if word1 == word2:
            return True

        # SprawdÅº wspÃ³lnÄ… dÅ‚ugoÅ›Ä‡
        if abs(len(word1) - len(word2)) > 2:
            return False

        # SprawdÅº wspÃ³lnÄ… czÄ™Å›Ä‡
        common_chars = set(word1) & set(word2)
        return len(common_chars) / max(len(set(word1)), len(set(word2))) > 0.6

    def get_stats(self) -> Dict[str, Any]:
        """Zwraca statystyki procesora NLP"""
        avg_processing_time = sum(self.stats["processing_times"]) / len(self.stats["processing_times"]) if self.stats["processing_times"] else 0.0

        return {
            "total_analyses": self.stats["total_analyses"],
            "cache_hits": self.stats["cache_hits"],
            "cache_size": len(self.analysis_cache),
            "avg_processing_time": avg_processing_time,
            "language_distribution": dict(self.stats["language_distribution"]),
            "models_loaded": list(self.models.keys())
        }

    async def batch_analyze(self, texts: List[str], batch_size: int = 10) -> List[NLPAnalysisResult]:
        """
        Przeprowadza analizÄ™ NLP dla wielu tekstÃ³w wsadowo

        Args:
            texts: Lista tekstÃ³w do analizy
            batch_size: Rozmiar wsadu

        Returns:
            List[NLPAnalysisResult]: Wyniki analizy dla wszystkich tekstÃ³w
        """
        results = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]

            # PrzetwÃ³rz wsad rÃ³wnolegle
            tasks = [self.analyze_text(text) for text in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in batch_results:
                if isinstance(result, Exception):
                    log_error(f"[NLP_PROCESSOR] BÅ‚Ä…d w analizie wsadowej: {result}")
                    # Dodaj pusty wynik dla bÅ‚Ä™dnego tekstu
                    results.append(NLPAnalysisResult(
                        text="",
                        language="error",
                        tokens=[],
                        entities=[],
                        sentiment={"polarity": 0.0, "subjectivity": 0.0, "emotions": {}, "intensity": 0.0},
                        key_phrases=[],
                        pos_tags=[],
                        dependencies=[],
                        readability_score=0.0,
                        processing_time=0.0
                    ))
                else:
                    results.append(result)

        return results

# Globalna instancja procesora NLP
_nlp_processor = None

def get_nlp_processor() -> NLPProcessor:
    """Pobierz globalnÄ… instancjÄ™ procesora NLP"""
    global _nlp_processor
    if _nlp_processor is None:
        _nlp_processor = NLPProcessor()
    return _nlp_processor

# Test funkcji
if __name__ == "__main__":
    async def test_nlp():
        """Test procesora NLP"""
        processor = get_nlp_processor()

        test_texts = [
            "To jest przykÅ‚adowy tekst w jÄ™zyku polskim. Zawiera rÃ³Å¼ne czÄ™Å›ci mowy i encje.",
            "Python to wspaniaÅ‚y jÄ™zyk programowania. UÅ¼ywam go do tworzenia aplikacji webowych.",
            "Sztuczna inteligencja rewolucjonizuje sposÃ³b, w jaki rozwiÄ…zujemy problemy."
        ]

        print("ğŸ§  TEST PROCESORA NLP")
        print("=" * 50)

        for i, text in enumerate(test_texts):
            print(f"\nğŸ“ Tekst {i+1}: {text[:60]}...")
            analysis = await processor.analyze_text(text)

            print(f"ğŸŒ JÄ™zyk: {analysis.language}")
            print(f"ğŸ·ï¸ Encje: {len(analysis.entities)}")
            print(f"ğŸ”‘ Frazy kluczowe: {analysis.key_phrases[:3]}")
            print(f"ğŸ˜Š Sentyment: {analysis.sentiment['polarity']:.2f}")
            print(f"ğŸ“– CzytelnoÅ›Ä‡: {analysis.readability_score:.2f}")
            print(f"âš¡ Czas: {analysis.processing_time:.3f}s")

        # Test ekstrakcji tematÃ³w
        topics = await processor.extract_topics(test_texts)
        print(f"\nğŸ“Š Tematy: {topics}")

        # Statystyki
        stats = processor.get_stats()
        print(f"\nğŸ“ˆ Statystyki: {stats}")

    # Uruchom test
    asyncio.run(test_nlp())