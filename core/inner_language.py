"""
 WASNY JZYK WEWNTRZNY (Inner Language Code)
==============================================

Ostatni element zaawansowanej architektury kognitywnej - wewntrzny jzyk semantyczny
dla szybszego przetwarzania myli i lepszych skojarze kontekstowych.

Autor: Zaawansowany System Kognitywny MRD
Data: 15 pa藕dziernika 2025
"""

import asyncio
import json
import time
import re
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Set, Union
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
from enum import Enum
import hashlib
import math
from itertools import combinations

from .config import *
from .llm import get_llm_client
from .memory import get_memory_manager
from .hierarchical_memory import get_hierarchical_memory
from .helpers import log_info, log_error, log_warning

class TokenType(Enum):
    """Typy token贸w w jzyku wewntrznym"""
    CONCEPT = "concept"         # Pojcie abstrakcyjne
    ENTITY = "entity"           # Konkretna jednostka
    RELATION = "relation"       # Relacja midzy elementami
    ACTION = "action"           # Akcja/czasownik
    PROPERTY = "property"       # Waciwo/cecha
    EMOTION = "emotion"         # Stan emocjonalny
    META = "meta"              # Meta-informacja
    PATTERN = "pattern"         # Wzorzec mylowy

class SemanticDimension(Enum):
    """Wymiary semantyczne token贸w"""
    ABSTRACTION = "abstraction"     # Poziom abstrakcji (0-1)
    CERTAINTY = "certainty"         # Pewno (0-1)
    IMPORTANCE = "importance"       # Wa偶no (0-1)
    TEMPORALITY = "temporality"     # Czasowo (-1 przeszo, 0 tera藕, 1 przyszo)
    EMOTIONALITY = "emotionality"   # Nacechowanie emocjonalne (-1 do 1)
    COMPLEXITY = "complexity"       # Zo偶ono pojciowa (0-1)
    NOVELTY = "novelty"            # Nowo (0-1)

@dataclass
class InnerToken:
    """Token w jzyku wewntrznym"""
    token_id: str
    surface_form: str           # Forma powierzchniowa w jzyku naturalnym
    semantic_core: str          # Rdze semantyczny
    token_type: TokenType
    dimensions: Dict[SemanticDimension, float]
    associations: List[str]     # ID powizanych token贸w
    activation_level: float     # Poziom aktywacji (0-1)
    creation_time: datetime
    usage_count: int
    context_patterns: List[str] # Wzorce kontekst贸w u偶ycia
    compression_ratio: float    # Stosunek kompresji vs pena forma

@dataclass
class SemanticCluster:
    """Klaster semantyczny token贸w"""
    cluster_id: str
    core_tokens: List[str]      # ID token贸w centralnych
    peripheral_tokens: List[str] # ID token贸w peryferyjnych
    cluster_theme: str          # G贸wny temat klastra
    coherence_score: float      # Sp贸jno klastra (0-1)
    activation_history: List[Tuple[datetime, float]]
    inter_cluster_links: Dict[str, float]  # Linki do innych klastr贸w

@dataclass
class ThoughtPattern:
    """Wzorzec mylowy"""
    pattern_id: str
    token_sequence: List[str]   # Sekwencja token贸w
    trigger_conditions: List[str]
    completion_probability: float
    usage_frequency: float
    effectiveness_score: float
    context_specificity: float

@dataclass
class InnerThought:
    """Myl w jzyku wewntrznym"""
    thought_id: str
    token_chain: List[str]      # acuch token贸w
    semantic_vector: List[float] # Wektor semantyczny
    compression_level: float    # Poziom kompresji (0-1)
    processing_time: float      # Czas przetwarzania
    confidence: float           # Pewno myli
    originality: float          # Oryginalno
    surface_translation: str    # Tumaczenie na jzyk naturalny

class InnerLanguageProcessor:
    """
     Procesor Wasnego Jzyka Wewntrznego
    
    Implementuje zaawansowany system jzykowy dla wewntrznego przetwarzania:
    - Tworzenie semantycznych token贸w i klastr贸w
    - Kompresja myli do form symbolicznych
    - Szybkie kojarzenia i pattern matching
    - Meta-jzykowe refleksje nad wasnym myleniem
    """
    
    def __init__(self):
        self.llm_client = get_llm_client()
        self.memory = get_memory_manager()
        self.hierarchical_memory = get_hierarchical_memory()
        
        # Sownik token贸w
        self.token_dictionary: Dict[str, InnerToken] = {}
        
        # Klastry semantyczne
        self.semantic_clusters: Dict[str, SemanticCluster] = {}
        
        # Wzorce mylowe
        self.thought_patterns: Dict[str, ThoughtPattern] = {}
        
        # Historia myli
        self.thought_history: deque = deque(maxlen=1000)
        
        # Sieci skojarzeniowe
        self.association_network: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))
        
        # Parametry systemu
        self.min_token_frequency = 3      # Min czstotliwo dla zachowania tokena
        self.max_tokens = 10000          # Max token贸w w sowniku
        self.cluster_threshold = 0.7     # Pr贸g podobiestwa dla klastrowania
        self.activation_decay = 0.9      # Tempo zanikania aktywacji
        self.compression_target = 0.3    # Docelowy poziom kompresji
        
        # Metryki wydajnoci
        self.performance_metrics = {
            "compression_ratio": 0.0,
            "association_speed": 0.0,
            "pattern_recognition_accuracy": 0.0,
            "translation_quality": 0.0,
            "total_tokens": 0,
            "active_clusters": 0,
            "thoughts_processed": 0
        }
        
        log_info("[INNER_LANGUAGE] Procesor Wasnego Jzyka Wewntrznego zainicjalizowany")
    
    async def process_natural_language_input(
        self,
        text: str,
        context: Dict[str, Any] = None
    ) -> InnerThought:
        """
        INTELIGENTNE przetwarzanie - zachowaj LLM ale zoptymalizuj dla real-time
        """

        try:
            start_time = time.time()

            # FAST CACHE CHECK - jeli ju偶 przetworzylimy podobne
            cache_key = hash(text[:50]) % 1000
            if hasattr(self, '_thought_cache') and cache_key in self._thought_cache:
                cached = self._thought_cache[cache_key]
                if cached['text'][:50] == text[:50]:  # Dokadne dopasowanie pocztku
                    return cached['thought']

            # SZYBKA TOKENIZACJA jako baza
            words = text.lower().split()
            base_tokens = [f"WORD_{word[:8]}" for word in words[:8]]

            # LLM tylko dla KLUCZOWYCH s贸w - nie dla caego tekstu!
            key_words = [word for word in words if len(word) > 3][:5]  # Max 5 s贸w

            if key_words:
                # MINI LLM CALL - tylko dla kluczowych s贸w
                mini_prompt = f"Kluczowe aspekty: {', '.join(key_words)}. Podaj 3 g贸wne kategorie znaczeniowe."

                try:
                    mini_response = await self.llm_client.chat_completion([{
                        "role": "system",
                        "content": "Jeste szybkim analizatorem semantycznym. Odpowiadaj bardzo kr贸tko."
                    }, {
                        "role": "user",
                        "content": mini_prompt
                    }], timeout=1.0)  # Timeout 1 sekunda!

                    # Parsuj kategorie
                    categories = mini_response.split(',')[:3]
                    enhanced_tokens = base_tokens[:5]  # Zachowaj podstaw

                    # Dodaj kategorie jako tokeny
                    for cat in categories:
                        cat_clean = cat.strip()[:10]
                        if cat_clean:
                            enhanced_tokens.append(f"CAT_{cat_clean}")

                except Exception:
                    # Fallback do podstawowej tokenizacji
                    enhanced_tokens = base_tokens
            else:
                enhanced_tokens = base_tokens

            # WEKTOR SEMANTYCZNY - mieszanka hash + kategorie
            semantic_vector = []

            # Hash s贸w
            for word in words[:6]:
                semantic_vector.append(hash(word) % 100 / 100.0)

            # Hash kategorii jeli s
            for token in enhanced_tokens[-3:]:
                semantic_vector.append(hash(token) % 100 / 100.0)

            # Wypenij do 64
            while len(semantic_vector) < 64:
                semantic_vector.append(0.5)

            semantic_vector = semantic_vector[:64]

            thought = InnerThought(
                thought_id=hashlib.md5(f"{text}_{time.time()}".encode()).hexdigest()[:12],
                token_chain=enhanced_tokens,
                semantic_vector=semantic_vector,
                compression_level=len(enhanced_tokens) / len(words) if words else 1.0,
                processing_time=time.time() - start_time,
                confidence=0.9,  # Wy偶sza pewno dziki LLM
                originality=0.6,  # rednia oryginalno
                surface_translation=text
            )

            # CACHE dla przyszych wywoa
            if not hasattr(self, '_thought_cache'):
                self._thought_cache = {}
            self._thought_cache[cache_key] = {
                'text': text,
                'thought': thought,
                'timestamp': time.time()
            }

            # Oczy stary cache (max 1000 wpis贸w)
            if len(self._thought_cache) > 1000:
                oldest_keys = sorted(self._thought_cache.keys(),
                                   key=lambda k: self._thought_cache[k]['timestamp'])[:100]
                for key in oldest_keys:
                    del self._thought_cache[key]

            return thought

        except Exception as e:
            log_error(f"[INNER_LANGUAGE] Bd: {e}")
            return await self._create_fallback_thought(text)

    async def _analyze_semantic_content(
        self, 
        text: str, 
        context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """Analizuj zawarto semantyczn tekstu"""
        
        analysis_prompt = f"""
        Przeanalizuj semantyczn struktur nastpujcego tekstu:
        
        TEKST: {text}
        KONTEKST: {json.dumps(context or {}, ensure_ascii=False)}
        
        Zidentyfikuj:
        1. Kluczowe pojcia (concepts)
        2. Konkretne jednostki (entities)  
        3. Relacje midzy elementami
        4. Akcje/czasowniki
        5. Waciwoci/cechy
        6. Stany emocjonalne
        7. Meta-informacje
        8. Wzorce mylowe
        
        Dla ka偶dego elementu okrel wymiary semantyczne:
        - Poziom abstrakcji (0-1)
        - Pewno (0-1)
        - Wa偶no (0-1)
        - Czasowo (-1 przeszo, 0 tera藕, 1 przyszo)
        - Nacechowanie emocjonalne (-1 do 1)
        - Zo偶ono (0-1)
        - Nowo (0-1)
        
        Format JSON:
        {{
          "concepts": [{{"text": "przykad", "dimensions": {{"abstraction": 0.5, "certainty": 0.8}}}},
          "entities": [{{"text": "osoba", "dimensions": {{"abstraction": 0.2, "certainty": 0.9}}}},
          "relations": [],
          "actions": [{{"text": "rozumie", "dimensions": {{"abstraction": 0.3, "certainty": 0.7}}}},
          "properties": [],
          "emotions": [],
          "meta": [],
          "patterns": []
        }}
        """
        
        try:
            analysis_response = await self.llm_client.chat_completion([{
                "role": "system",
                "content": "Jeste ekspertem w analizie semantycznej. Analizujesz struktur znaczeniow tekst贸w."
            }, {
                "role": "user", 
                "content": analysis_prompt
            }])
            
            # Parsuj JSON
            try:
                # Usu markdown code blocks jeli s
                clean_response = analysis_response.strip()
                if clean_response.startswith('```json'):
                    clean_response = clean_response[7:]
                if clean_response.startswith('```'):
                    clean_response = clean_response[3:]
                if clean_response.endswith('```'):
                    clean_response = clean_response[:-3]
                clean_response = clean_response.strip()
                
                analysis = json.loads(clean_response)
                
                # Dodatkowa walidacja struktury JSON
                if not isinstance(analysis, dict):
                    raise ValueError("Odpowied藕 nie jest obiektem JSON")
                
                # Sprawd藕 czy zawiera wymagane klucze
                required_keys = ["concepts", "entities", "actions"]
                if not any(key in analysis for key in required_keys):
                    raise ValueError("Brak wymaganych kluczy w odpowiedzi JSON")
                
                return analysis
                
            except (json.JSONDecodeError, ValueError) as e:
                log_warning(f"[INNER_LANGUAGE] Nieprawidowy JSON z LLM: {e}, odpowied藕: {analysis_response[:500]}...")
                return self._fallback_semantic_analysis(text)
                
        except Exception as e:
            log_error(f"[INNER_LANGUAGE] Bd analizy semantycznej: {e}")
            return self._fallback_semantic_analysis(text)
    
    def _fallback_semantic_analysis(self, text: str) -> Dict[str, Any]:
        """Fallback analiza semantyczna"""
        
        words = text.lower().split()
        
        # Prosta klasyfikacja na podstawie POS tagging (symulacja)
        concepts = [word for word in words if len(word) > 4][:5]
        entities = [word for word in words if word.istitle()]
        actions = [word for word in words if word.endswith(('i', 'a', 'owa'))]
        
        return {
            "concepts": [{"text": concept, "dimensions": self._default_dimensions()} for concept in concepts],
            "entities": [{"text": entity, "dimensions": self._default_dimensions()} for entity in entities],
            "relations": [],
            "actions": [{"text": action, "dimensions": self._default_dimensions()} for action in actions],
            "properties": [],
            "emotions": [],
            "meta": [],
            "patterns": []
        }
    
    def _validate_dimensions(self, dimensions: Dict[str, Any]) -> Dict[str, float]:
        """Walidacja i normalizacja wymiar贸w semantycznych"""
        
        validated = {}
        
        # Sprawd藕 ka偶dy mo偶liwy wymiar
        for dim_name in ["abstraction", "certainty", "importance", "temporality", "emotionality", "complexity", "novelty"]:
            value = dimensions.get(dim_name)
            
            # Sprawd藕 czy warto nie jest None
            if value is None:
                log_warning(f"[INNER_LANGUAGE] Dimension {dim_name} is None, using default")
                validated[dim_name] = self._default_dimensions()[dim_name]
                continue
            
            # Sprawd藕 typ wartoci
            if isinstance(value, str):
                try:
                    # Usu ewentualne cudzysowy i dodatkowe znaki
                    clean_value = value.strip().strip('"').strip("'")
                    log_warning(f"[INNER_LANGUAGE] String dimension {dim_name}: '{value}' -> '{clean_value}'")
                    # Jeli zawiera JSON-like content, wycignij pierwsz liczb
                    import re
                    numbers = re.findall(r'[0-9.]+', clean_value)
                    if numbers:
                        validated[dim_name] = float(numbers[0])
                    else:
                        validated[dim_name] = self._default_dimensions()[dim_name]
                except (ValueError, IndexError) as e:
                    log_warning(f"[INNER_LANGUAGE] Error parsing dimension {dim_name}: {e}")
                    validated[dim_name] = self._default_dimensions()[dim_name]
            # Jeli warto jest liczb, upewnij si 偶e jest float
            elif isinstance(value, (int, float)):
                validated[dim_name] = float(value)
            # W przeciwnym razie u偶yj domylnej
            else:
                log_warning(f"[INNER_LANGUAGE] Invalid dimension type {dim_name}: {type(value)} = {value}")
                validated[dim_name] = self._default_dimensions()[dim_name]
        
        return validated
    
    def _default_dimensions(self):
        """Zwraca domylne wymiary semantyczne"""
        return {
            "abstraction": 0.5,
            "certainty": 0.5,
            "importance": 0.4,
            "temporality": 0.0,
            "emotionality": 0.2,
            "complexity": 0.3,
            "novelty": 0.1
        }
    
    async def _map_to_inner_tokens(
        self, 
        text: str, 
        semantic_analysis: Dict[str, Any]
    ) -> List[str]:
        """Mapuj elementy semantyczne na tokeny wewntrzne"""
        
        token_chain = []
        
        # Przejd藕 przez wszystkie kategorie semantyczne
        for category, elements in semantic_analysis.items():
            if category in ["concepts", "entities", "relations", "actions", "properties", "emotions", "meta", "patterns"]:
                # Mapuj kategori na TokenType
                type_mapping = {
                    "concepts": TokenType.CONCEPT,
                    "entities": TokenType.ENTITY,
                    "relations": TokenType.RELATION,
                    "actions": TokenType.ACTION,
                    "properties": TokenType.PROPERTY,
                    "emotions": TokenType.EMOTION,
                    "meta": TokenType.META,
                    "patterns": TokenType.PATTERN
                }
                token_type = type_mapping.get(category, TokenType.CONCEPT)
                
                for element in elements:
                    if isinstance(element, dict) and "text" in element:
                        # Walidacja dimensions
                        dimensions = element.get("dimensions", self._default_dimensions())
                        validated_dimensions = self._validate_dimensions(dimensions)
                        
                        token_id = await self._get_or_create_token(
                            surface_form=element["text"],
                            token_type=token_type,
                            dimensions=validated_dimensions
                        )
                        token_chain.append(token_id)
        
        return token_chain
    
    async def _get_or_create_token(
        self, 
        surface_form: str, 
        token_type: TokenType,
        dimensions: Dict[str, float]
    ) -> str:
        """Pobierz istniejcy token lub stw贸rz nowy"""
        
        # Generuj rdze semantyczny
        semantic_core = await self._extract_semantic_core(surface_form, token_type)
        
        # Sprawd藕 czy token ju偶 istnieje
        for token_id, token in self.token_dictionary.items():
            if (token.semantic_core == semantic_core and 
                token.token_type == token_type):
                
                # Aktualizuj istniejcy token
                token.usage_count += 1
                token.activation_level = min(token.activation_level + 0.1, 1.0)
                
                # Urednij wymiary
                for dim_name, dim_value in dimensions.items():
                    if hasattr(SemanticDimension, dim_name.upper()):
                        dim_enum = getattr(SemanticDimension, dim_name.upper())
                        if dim_enum in token.dimensions:
                            token.dimensions[dim_enum] = (token.dimensions[dim_enum] + dim_value) / 2
                        else:
                            token.dimensions[dim_enum] = dim_value
                
                return token_id
        
        # Stw贸rz nowy token
        token_id = hashlib.md5(f"{semantic_core}_{token_type.value}_{time.time()}".encode()).hexdigest()[:8]
        
        # Konwertuj wymiary string -> enum
        enum_dimensions = {}
        for dim_name, dim_value in dimensions.items():
            try:
                dim_enum = getattr(SemanticDimension, dim_name.upper())
                enum_dimensions[dim_enum] = dim_value
            except AttributeError:
                continue
        
        new_token = InnerToken(
            token_id=token_id,
            surface_form=surface_form,
            semantic_core=semantic_core,
            token_type=token_type,
            dimensions=enum_dimensions,
            associations=[],
            activation_level=0.5,
            creation_time=datetime.now(),
            usage_count=1,
            context_patterns=[],
            compression_ratio=len(semantic_core) / len(surface_form) if surface_form else 1.0
        )
        
        self.token_dictionary[token_id] = new_token
        
        # Ograniczenia sownika
        await self._manage_dictionary_size()
        
        return token_id
    
    async def _extract_semantic_core(self, surface_form: str, token_type: TokenType) -> str:
        """Wydobd藕 rdze semantyczny z formy powierzchniowej"""
        
        # Prosta heurystyka - mo偶na rozbudowa o stemming/lemmatyzacj
        core = surface_form.lower().strip()
        
        # Usu koc贸wki fleksyjne (uproszczone)
        if token_type == TokenType.ACTION:
            for suffix in ['owa', 'i', 'a', 'e', 'n']:
                if core.endswith(suffix):
                    core = core[:-len(suffix)]
                    break
        elif token_type == TokenType.PROPERTY:
            for suffix in ['owy', 'ny', 'ski', 'cki']:
                if core.endswith(suffix):
                    core = core[:-len(suffix)]
                    break
        
        return core[:20]  # Ogranicz dugo
    
    async def _compress_token_chain(self, token_chain: List[str]) -> List[str]:
        """Skompresuj acuch token贸w"""
        
        if len(token_chain) <= 3:
            return token_chain
        
        # METODA 1: Usu tokeny o niskiej aktywacji
        filtered_chain = []
        for token_id in token_chain:
            if token_id in self.token_dictionary:
                token = self.token_dictionary[token_id]
                if token.activation_level > 0.3:  # Threshold aktywacji
                    filtered_chain.append(token_id)
        
        # METODA 2: Zastp sekwencje wzorcami
        pattern_compressed = await self._apply_pattern_compression(filtered_chain)
        
        # METODA 3: Cluster token贸w podobnych
        cluster_compressed = await self._apply_cluster_compression(pattern_compressed)
        
        return cluster_compressed
    
    async def _apply_pattern_compression(self, token_chain: List[str]) -> List[str]:
        """Zastosuj kompresj wzorcami"""
        
        # Sprawd藕 znane wzorce
        for pattern in self.thought_patterns.values():
            if (len(pattern.token_sequence) <= len(token_chain) and
                pattern.usage_frequency > 0.1):
                
                # Sprawd藕 czy wzorzec pasuje
                for i in range(len(token_chain) - len(pattern.token_sequence) + 1):
                    subsequence = token_chain[i:i + len(pattern.token_sequence)]
                    
                    if self._sequences_match(subsequence, pattern.token_sequence, similarity_threshold=0.8):
                        # Zastp wzorcem
                        compressed = (token_chain[:i] + 
                                    [f"PATTERN_{pattern.pattern_id}"] + 
                                    token_chain[i + len(pattern.token_sequence):])
                        return await self._apply_pattern_compression(compressed)
        
        return token_chain
    
    def _sequences_match(self, seq1: List[str], seq2: List[str], similarity_threshold: float = 0.8) -> bool:
        """Sprawd藕 czy sekwencje token贸w s podobne"""
        
        if len(seq1) != len(seq2):
            return False
        
        matches = sum(1 for a, b in zip(seq1, seq2) if a == b)
        similarity = matches / len(seq1)
        
        return similarity >= similarity_threshold
    
    async def _apply_cluster_compression(self, token_chain: List[str]) -> List[str]:
        """Zastosuj kompresj klastrami"""
        
        # Znajd藕 tokeny nale偶ce do tego samego klastra
        compressed_chain = []
        i = 0
        
        while i < len(token_chain):
            current_token = token_chain[i]
            
            # Sprawd藕 czy nastpne tokeny nale偶 do tego samego klastra
            cluster_id = self._find_token_cluster(current_token)
            if cluster_id:
                cluster_tokens = [current_token]
                j = i + 1
                
                while j < len(token_chain) and j < i + 3:  # Max 3 tokeny w grupie
                    if self._find_token_cluster(token_chain[j]) == cluster_id:
                        cluster_tokens.append(token_chain[j])
                        j += 1
                    else:
                        break
                
                if len(cluster_tokens) > 1:
                    # Zastp grup reprezentantem klastra
                    compressed_chain.append(f"CLUSTER_{cluster_id}")
                    i = j
                else:
                    compressed_chain.append(current_token)
                    i += 1
            else:
                compressed_chain.append(current_token)
                i += 1
        
        return compressed_chain
    
    def _find_token_cluster(self, token_id: str) -> Optional[str]:
        """Znajd藕 klaster dla tokena"""
        
        for cluster_id, cluster in self.semantic_clusters.items():
            if token_id in cluster.core_tokens or token_id in cluster.peripheral_tokens:
                return cluster_id
        
        return None
    
    async def _generate_semantic_vector(self, token_chain: List[str]) -> List[float]:
        """Generuj wektor semantyczny dla acucha token贸w"""
        
        if not token_chain:
            return [0.0] * 64  # Domylny wymiar wektora
        
        # METODA 1: rednia wa偶ona wektor贸w token贸w
        vector = [0.0] * 64
        total_weight = 0.0
        
        for token_id in token_chain:
            if token_id.startswith("PATTERN_") or token_id.startswith("CLUSTER_"):
                # Specjalne traktowanie wzorc贸w i klastr贸w
                token_weight = 1.0
                token_vector = self._generate_pattern_cluster_vector(token_id)
            elif token_id in self.token_dictionary:
                token = self.token_dictionary[token_id]
                token_weight = token.activation_level * token.dimensions.get(SemanticDimension.IMPORTANCE, 0.5)
                token_vector = self._token_to_vector(token)
            else:
                continue
            
            # Dodaj wa偶ony wkad tokena
            for i in range(len(vector)):
                vector[i] += token_vector[i] * token_weight
            
            total_weight += token_weight
        
        # Normalizuj
        if total_weight > 0:
            vector = [v / total_weight for v in vector]
        
        return vector
    
    def _token_to_vector(self, token: InnerToken) -> List[float]:
        """Konwertuj token na wektor"""
        
        # Bazowy wektor na podstawie typu tokena
        type_vectors = {
            TokenType.CONCEPT: [1, 0, 0, 0, 0, 0, 0, 0],
            TokenType.ENTITY: [0, 1, 0, 0, 0, 0, 0, 0],
            TokenType.RELATION: [0, 0, 1, 0, 0, 0, 0, 0],
            TokenType.ACTION: [0, 0, 0, 1, 0, 0, 0, 0],
            TokenType.PROPERTY: [0, 0, 0, 0, 1, 0, 0, 0],
            TokenType.EMOTION: [0, 0, 0, 0, 0, 1, 0, 0],
            TokenType.META: [0, 0, 0, 0, 0, 0, 1, 0],
            TokenType.PATTERN: [0, 0, 0, 0, 0, 0, 0, 1]
        }
        
        base_vector = type_vectors.get(token.token_type, [0] * 8)
        
        # Dodaj wymiary semantyczne
        dimensions_vector = [
            token.dimensions.get(dim, 0.0) for dim in SemanticDimension.__members__.values()
        ]
        
        # Dodaj hash rdzenia semantycznego jako pseudo-embedding
        core_hash = hash(token.semantic_core) % 1000000
        hash_vector = [(core_hash >> i) & 1 for i in range(20)]  # 20-bitowy hash
        
        # Kombinuj wszystkie skadniki
        full_vector = base_vector + dimensions_vector + hash_vector
        
        # Dopenij do 64 wymiar贸w
        while len(full_vector) < 64:
            full_vector.append(0.0)
        
        return full_vector[:64]
    
    def _generate_pattern_cluster_vector(self, special_token: str) -> List[float]:
        """Generuj wektor dla wzorc贸w i klastr贸w"""
        
        if special_token.startswith("PATTERN_"):
            pattern_id = special_token.replace("PATTERN_", "")
            if pattern_id in self.thought_patterns:
                pattern = self.thought_patterns[pattern_id]
                # rednia wektor贸w token贸w w wzorcu
                vectors = []
                for token_id in pattern.token_sequence:
                    if token_id in self.token_dictionary:
                        vectors.append(self._token_to_vector(self.token_dictionary[token_id]))
                
                if vectors:
                    return [sum(v[i] for v in vectors) / len(vectors) for i in range(64)]
        
        elif special_token.startswith("CLUSTER_"):
            cluster_id = special_token.replace("CLUSTER_", "")
            if cluster_id in self.semantic_clusters:
                cluster = self.semantic_clusters[cluster_id]
                # rednia wektor贸w token贸w w klastrze
                vectors = []
                for token_id in cluster.core_tokens + cluster.peripheral_tokens:
                    if token_id in self.token_dictionary:
                        vectors.append(self._token_to_vector(self.token_dictionary[token_id]))
                
                if vectors:
                    return [sum(v[i] for v in vectors) / len(vectors) for i in range(64)]
        
        return [0.0] * 64
    
    async def _calculate_thought_confidence(
        self, 
        token_chain: List[str], 
        semantic_vector: List[float]
    ) -> float:
        """Oblicz pewno myli"""
        
        confidence = 0.0
        
        # Skadnik 1: Jako token贸w
        token_quality = 0.0
        valid_tokens = 0
        
        for token_id in token_chain:
            if token_id in self.token_dictionary:
                token = self.token_dictionary[token_id]
                token_quality += token.dimensions.get(SemanticDimension.CERTAINTY, 0.5)
                valid_tokens += 1
        
        if valid_tokens > 0:
            confidence += (token_quality / valid_tokens) * 0.4
        
        # Skadnik 2: Sp贸jno semantyczna
        vector_magnitude = math.sqrt(sum(v**2 for v in semantic_vector))
        if vector_magnitude > 0:
            confidence += min(vector_magnitude / 10.0, 0.3)  # Normalizacja
        
        # Skadnik 3: Rozpoznane wzorce
        pattern_bonus = len([t for t in token_chain if t.startswith("PATTERN_")]) * 0.1
        confidence += min(pattern_bonus, 0.3)
        
        return min(confidence, 1.0)
    
    async def _assess_thought_originality(self, token_chain: List[str]) -> float:
        """Oce oryginalno myli"""
        
        originality = 0.0
        
        # Skadnik 1: Nowo token贸w
        novelty_sum = 0.0
        valid_tokens = 0
        
        for token_id in token_chain:
            if token_id in self.token_dictionary:
                token = self.token_dictionary[token_id]
                novelty_sum += token.dimensions.get(SemanticDimension.NOVELTY, 0.5)
                valid_tokens += 1
        
        if valid_tokens > 0:
            originality += (novelty_sum / valid_tokens) * 0.5
        
        # Skadnik 2: Unikalna kombinacja
        chain_signature = "_".join(sorted(token_chain))
        is_unique = True
        
        for past_thought in list(self.thought_history)[-100:]:  # Sprawd藕 ostatnie 100
            past_signature = "_".join(sorted(past_thought.token_chain))
            if chain_signature == past_signature:
                is_unique = False
                break
        
        if is_unique:
            originality += 0.5
        
        return min(originality, 1.0)
    
    async def _update_association_networks(self, token_chain: List[str]):
        """Aktualizuj sieci skojarzeniowe"""
        
        # Aktualizuj powizania midzy tokenami w acuchu
        for i, token1 in enumerate(token_chain):
            for j, token2 in enumerate(token_chain):
                if i != j and token1 in self.token_dictionary and token2 in self.token_dictionary:
                    # Sia skojarze maleje z odlegoci
                    distance = abs(i - j)
                    association_strength = 1.0 / (distance + 1) * 0.1
                    
                    self.association_network[token1][token2] += association_strength
                    
                    # Aktualizuj listy skojarze w tokenach
                    token1_obj = self.token_dictionary[token1]
                    if token2 not in token1_obj.associations:
                        token1_obj.associations.append(token2)
    
    async def _manage_dictionary_size(self):
        """Zarzdzaj rozmiarem sownika token贸w"""
        
        if len(self.token_dictionary) > self.max_tokens:
            # Usu tokeny o najni偶szej czstotliwoci i aktywacji
            tokens_by_score = []
            
            for token_id, token in self.token_dictionary.items():
                score = token.usage_count * token.activation_level
                tokens_by_score.append((score, token_id))
            
            # Sortuj i usu najsabsze
            tokens_by_score.sort()
            tokens_to_remove = tokens_by_score[:len(self.token_dictionary) - self.max_tokens + 100]
            
            for _, token_id in tokens_to_remove:
                del self.token_dictionary[token_id]
                
                # Usu z sieci skojarze
                if token_id in self.association_network:
                    del self.association_network[token_id]
                
                for other_token in self.association_network:
                    if token_id in self.association_network[other_token]:
                        del self.association_network[other_token][token_id]
    
    async def _create_fallback_thought(self, text: str) -> InnerThought:
        """Stw贸rz prost myl jako fallback"""
        
        return InnerThought(
            thought_id=hashlib.md5(f"fallback_{text}_{time.time()}".encode()).hexdigest()[:12],
            token_chain=[f"FALLBACK_{word}" for word in text.split()[:5]],
            semantic_vector=[0.5] * 64,
            compression_level=0.1,
            processing_time=0.01,
            confidence=0.3,
            originality=0.1,
            surface_translation=text
        )
    
    async def translate_to_natural_language(
        self, 
        inner_thought: InnerThought,
        style: str = "natural",
        max_length: int = 500
    ) -> str:
        """
        Przetumacz myl z jzyka wewntrznego na naturalny
        
        Args:
            inner_thought: Myl w jzyku wewntrznym
            style: Styl tumaczenia ("natural", "technical", "poetic")
            max_length: Maksymalna dugo
            
        Returns:
            str: Tekst w jzyku naturalnym
        """
        
        try:
            # Dekompresuj tokeny na powierzchnie jzykowe
            surface_elements = []
            
            for token_id in inner_thought.token_chain:
                if token_id.startswith("PATTERN_"):
                    # Rozwi wzorzec
                    pattern_id = token_id.replace("PATTERN_", "")
                    if pattern_id in self.thought_patterns:
                        pattern = self.thought_patterns[pattern_id]
                        for sub_token in pattern.token_sequence:
                            if sub_token in self.token_dictionary:
                                surface_elements.append(self.token_dictionary[sub_token].surface_form)
                    
                elif token_id.startswith("CLUSTER_"):
                    # Rozwi klaster
                    cluster_id = token_id.replace("CLUSTER_", "")
                    if cluster_id in self.semantic_clusters:
                        cluster = self.semantic_clusters[cluster_id]
                        surface_elements.append(f"[{cluster.cluster_theme}]")
                
                elif token_id in self.token_dictionary:
                    token = self.token_dictionary[token_id]
                    surface_elements.append(token.surface_form)
                
                else:
                    # Fallback tokeny
                    if token_id.startswith("FALLBACK_"):
                        surface_elements.append(token_id.replace("FALLBACK_", ""))
            
            # U偶yj LLM do stylizacji i naturalizacji
            raw_text = " ".join(surface_elements)
            
            stylization_prompt = f"""
            Przekszta nastpujce elementy semantyczne na pynny tekst w jzyku polskim:
            
            ELEMENTY: {raw_text}
            STYL: {style}
            KONTEKST: Kompresja myli: {inner_thought.compression_level:.2f}, Pewno: {inner_thought.confidence:.2f}
            
            Wytyczne:
            1. Zachowaj znaczenie wszystkich element贸w
            2. U偶yj stylu: {style}
            3. Maksymalna dugo: {max_length} znak贸w
            4. Naturalny, pynny jzyk polski
            
            Zwr贸 tylko przetumaczony tekst.
            """
            
            translated = await self.llm_client.chat_completion([{
                "role": "system",
                "content": f"Jeste ekspertem w tumaczeniu jzyka wewntrznego na naturalny. Specjalizujesz si w stylu: {style}."
            }, {
                "role": "user",
                "content": stylization_prompt
            }])
            
            # Ogranicz dugo
            if len(translated) > max_length:
                translated = translated[:max_length-3] + "..."
            
            return translated
            
        except Exception as e:
            log_error(f"[INNER_LANGUAGE] Bd tumaczenia: {e}")
            return " ".join(surface_elements) if surface_elements else inner_thought.surface_translation
    
    async def find_associations(
        self, 
        query_tokens: List[str], 
        max_associations: int = 10,
        min_strength: float = 0.1
    ) -> List[Tuple[str, float]]:
        """
        Znajd藕 skojarzenia dla zestawu token贸w
        
        Args:
            query_tokens: Lista token贸w zapytania
            max_associations: Maksymalna liczba skojarze
            min_strength: Minimalna sia skojarzenia
            
        Returns:
            List[Tuple[str, float]]: Lista (token_id, sia_skojarzenia)
        """
        
        associations = defaultdict(float)
        
        for query_token in query_tokens:
            if query_token in self.association_network:
                for associated_token, strength in self.association_network[query_token].items():
                    if strength >= min_strength:
                        associations[associated_token] += strength
        
        # Sortuj i ogranicz
        sorted_associations = sorted(associations.items(), key=lambda x: x[1], reverse=True)
        return sorted_associations[:max_associations]
    
    async def discover_thought_patterns(
        self, 
        min_frequency: int = 3,
        min_effectiveness: float = 0.6
    ) -> List[ThoughtPattern]:
        """
        Odkryj wzorce mylowe z historii
        
        Args:
            min_frequency: Minimalna czstotliwo wzorca
            min_effectiveness: Minimalna skuteczno
            
        Returns:
            List[ThoughtPattern]: Lista odkrytych wzorc贸w
        """
        
        patterns = []
        
        # Analizuj sekwencje token贸w w historii myli
        sequence_counts = defaultdict(int)
        
        for thought in self.thought_history:
            # Generuj wszystkie subsequencje dugoci 2-4
            for length in range(2, min(5, len(thought.token_chain) + 1)):
                for i in range(len(thought.token_chain) - length + 1):
                    subsequence = tuple(thought.token_chain[i:i + length])
                    sequence_counts[subsequence] += 1
        
        # Filtruj wzorce wedug czstotliwoci
        for sequence, count in sequence_counts.items():
            if count >= min_frequency:
                # Oblicz skuteczno wzorca
                effectiveness = await self._calculate_pattern_effectiveness(list(sequence))
                
                if effectiveness >= min_effectiveness:
                    pattern = ThoughtPattern(
                        pattern_id=hashlib.md5(str(sequence).encode()).hexdigest()[:8],
                        token_sequence=list(sequence),
                        trigger_conditions=[],  # TODO: okrel warunki
                        completion_probability=count / len(self.thought_history),
                        usage_frequency=count / len(self.thought_history),
                        effectiveness_score=effectiveness,
                        context_specificity=0.5  # TODO: oblicz
                    )
                    patterns.append(pattern)
                    
                    # Dodaj do sownika wzorc贸w
                    self.thought_patterns[pattern.pattern_id] = pattern
        
        log_info(f"[INNER_LANGUAGE] Odkryto {len(patterns)} nowych wzorc贸w mylowych")
        return patterns
    
    async def _calculate_pattern_effectiveness(self, token_sequence: List[str]) -> float:
        """Oblicz skuteczno wzorca mylowego"""
        
        # Prosta heurystyka - mo偶na rozbudowa
        effectiveness = 0.0
        
        # Bonus za r贸偶norodno typ贸w token贸w
        token_types = set()
        for token_id in token_sequence:
            if token_id in self.token_dictionary:
                token = self.token_dictionary[token_id]
                token_types.add(token.token_type)
        
        effectiveness += len(token_types) / len(TokenType) * 0.4
        
        # Bonus za wysok aktywacj token贸w
        avg_activation = 0.0
        valid_tokens = 0
        
        for token_id in token_sequence:
            if token_id in self.token_dictionary:
                token = self.token_dictionary[token_id]
                avg_activation += token.activation_level
                valid_tokens += 1
        
        if valid_tokens > 0:
            effectiveness += (avg_activation / valid_tokens) * 0.6
        
        return min(effectiveness, 1.0)
    
    async def cluster_semantic_tokens(
        self, 
        similarity_threshold: float = 0.7,
        min_cluster_size: int = 3
    ) -> List[SemanticCluster]:
        """
        Klastruj tokeny semantycznie podobne
        
        Args:
            similarity_threshold: Pr贸g podobiestwa
            min_cluster_size: Minimalna wielko klastra
            
        Returns:
            List[SemanticCluster]: Lista klastr贸w
        """
        
        try:
            log_info("[INNER_LANGUAGE] Klastrowanie token贸w semantycznych...")
            
            # Oblicz macierz podobiestw
            token_ids = list(self.token_dictionary.keys())
            similarity_matrix = {}
            
            for i, token_id1 in enumerate(token_ids):
                similarity_matrix[token_id1] = {}
                for j, token_id2 in enumerate(token_ids):
                    if i <= j:
                        similarity = await self._calculate_token_similarity(token_id1, token_id2)
                        similarity_matrix[token_id1][token_id2] = similarity
                        if token_id2 not in similarity_matrix:
                            similarity_matrix[token_id2] = {}
                        similarity_matrix[token_id2][token_id1] = similarity
            
            # Prosty algorytm klastrowania (mo偶na zastpi zaawansowanymi)
            clusters = []
            used_tokens = set()
            
            for token_id in token_ids:
                if token_id in used_tokens:
                    continue
                
                # Znajd藕 podobne tokeny
                cluster_tokens = [token_id]
                used_tokens.add(token_id)
                
                for other_token in token_ids:
                    if (other_token not in used_tokens and 
                        similarity_matrix[token_id][other_token] >= similarity_threshold):
                        cluster_tokens.append(other_token)
                        used_tokens.add(other_token)
                
                # Stw贸rz klaster jeli wystarczajco du偶y
                if len(cluster_tokens) >= min_cluster_size:
                    cluster_theme = await self._determine_cluster_theme(cluster_tokens)
                    
                    cluster = SemanticCluster(
                        cluster_id=hashlib.md5(f"cluster_{time.time()}_{len(clusters)}".encode()).hexdigest()[:8],
                        core_tokens=cluster_tokens[:3],  # Najbardziej reprezentatywne
                        peripheral_tokens=cluster_tokens[3:],
                        cluster_theme=cluster_theme,
                        coherence_score=await self._calculate_cluster_coherence(cluster_tokens),
                        activation_history=[(datetime.now(), 0.5)],
                        inter_cluster_links={}
                    )
                    
                    clusters.append(cluster)
                    self.semantic_clusters[cluster.cluster_id] = cluster
            
            log_info(f"[INNER_LANGUAGE] Utworzono {len(clusters)} klastr贸w semantycznych")
            return clusters
            
        except Exception as e:
            log_error(f"[INNER_LANGUAGE] Bd klastrowania: {e}")
            return []
    
    async def _calculate_token_similarity(self, token_id1: str, token_id2: str) -> float:
        """Oblicz podobiestwo midzy tokenami"""
        
        if token_id1 == token_id2:
            return 1.0
        
        if token_id1 not in self.token_dictionary or token_id2 not in self.token_dictionary:
            return 0.0
        
        token1 = self.token_dictionary[token_id1]
        token2 = self.token_dictionary[token_id2]
        
        similarity = 0.0
        
        # Skadnik 1: Typ tokena
        if token1.token_type == token2.token_type:
            similarity += 0.3
        
        # Skadnik 2: Podobiestwo wymiar贸w semantycznych
        dim_similarities = []
        for dim in SemanticDimension.__members__.values():
            val1 = token1.dimensions.get(dim, 0.5)
            val2 = token2.dimensions.get(dim, 0.5)
            dim_sim = 1.0 - abs(val1 - val2)
            dim_similarities.append(dim_sim)
        
        similarity += sum(dim_similarities) / len(dim_similarities) * 0.4
        
        # Skadnik 3: Rdze semantyczny
        core_sim = self._string_similarity(token1.semantic_core, token2.semantic_core)
        similarity += core_sim * 0.2
        
        # Skadnik 4: Skojarzenia
        if token_id2 in token1.associations or token_id1 in token2.associations:
            similarity += 0.1
        
        return min(similarity, 1.0)
    
    def _string_similarity(self, str1: str, str2: str) -> float:
        """Oblicz podobiestwo string贸w"""
        
        if not str1 or not str2:
            return 0.0
        
        # Prosta miara Jaccarda na poziomie znak贸w
        set1 = set(str1.lower())
        set2 = set(str2.lower())
        
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        return intersection / union if union > 0 else 0.0
    
    async def _determine_cluster_theme(self, token_ids: List[str]) -> str:
        """Okrel g贸wny temat klastra"""
        
        surface_forms = []
        token_types = defaultdict(int)
        
        for token_id in token_ids:
            if token_id in self.token_dictionary:
                token = self.token_dictionary[token_id]
                surface_forms.append(token.surface_form)
                token_types[token.token_type.value] += 1
        
        # Najczstszy typ tokena
        dominant_type = max(token_types.items(), key=lambda x: x[1])[0] if token_types else "general"
        
        # Pierwszy element jako reprezentant
        representative = surface_forms[0] if surface_forms else "unknown"
        
        return f"{dominant_type}_{representative}"[:50]
    
    async def _calculate_cluster_coherence(self, token_ids: List[str]) -> float:
        """Oblicz sp贸jno klastra"""
        
        if len(token_ids) < 2:
            return 1.0
        
        similarities = []
        
        for i, token_id1 in enumerate(token_ids):
            for j, token_id2 in enumerate(token_ids):
                if i < j:
                    similarity = await self._calculate_token_similarity(token_id1, token_id2)
                    similarities.append(similarity)
        
        return sum(similarities) / len(similarities) if similarities else 0.0
    
    async def get_inner_language_report(self) -> Dict[str, Any]:
        """Pobierz raport systemu jzyka wewntrznego"""
        
        # Aktualizuj metryki
        self.performance_metrics["total_tokens"] = len(self.token_dictionary)
        self.performance_metrics["active_clusters"] = len(self.semantic_clusters)
        
        # Analiza token贸w wedug typ贸w
        token_type_distribution = defaultdict(int)
        for token in self.token_dictionary.values():
            token_type_distribution[token.token_type.value] += 1
        
        # Top tokeny wedug aktywacji
        top_tokens = sorted(
            self.token_dictionary.values(),
            key=lambda t: t.activation_level * t.usage_count,
            reverse=True
        )[:10]
        
        # Najsilniejsze skojarzenia
        top_associations = []
        for token1, assocs in list(self.association_network.items())[:5]:
            for token2, strength in sorted(assocs.items(), key=lambda x: x[1], reverse=True)[:3]:
                if token1 in self.token_dictionary and token2 in self.token_dictionary:
                    top_associations.append({
                        "token1": self.token_dictionary[token1].surface_form,
                        "token2": self.token_dictionary[token2].surface_form,
                        "strength": strength
                    })
        
        report = {
            "performance_metrics": self.performance_metrics,
            "token_statistics": {
                "total_tokens": len(self.token_dictionary),
                "type_distribution": dict(token_type_distribution),
                "avg_activation": sum(t.activation_level for t in self.token_dictionary.values()) / len(self.token_dictionary) if self.token_dictionary else 0,
                "avg_usage": sum(t.usage_count for t in self.token_dictionary.values()) / len(self.token_dictionary) if self.token_dictionary else 0
            },
            "top_tokens": [
                {
                    "surface_form": token.surface_form,
                    "type": token.token_type.value,
                    "activation": token.activation_level,
                    "usage_count": token.usage_count,
                    "associations": len(token.associations)
                }
                for token in top_tokens
            ],
            "clustering": {
                "total_clusters": len(self.semantic_clusters),
                "avg_cluster_size": sum(len(c.core_tokens) + len(c.peripheral_tokens) for c in self.semantic_clusters.values()) / len(self.semantic_clusters) if self.semantic_clusters else 0,
                "avg_coherence": sum(c.coherence_score for c in self.semantic_clusters.values()) / len(self.semantic_clusters) if self.semantic_clusters else 0
            },
            "patterns": {
                "total_patterns": len(self.thought_patterns),
                "avg_effectiveness": sum(p.effectiveness_score for p in self.thought_patterns.values()) / len(self.thought_patterns) if self.thought_patterns else 0,
                "avg_frequency": sum(p.usage_frequency for p in self.thought_patterns.values()) / len(self.thought_patterns) if self.thought_patterns else 0
            },
            "top_associations": top_associations,
            "recent_thoughts": [
                {
                    "compression_level": thought.compression_level,
                    "confidence": thought.confidence,
                    "originality": thought.originality,
                    "processing_time": thought.processing_time,
                    "tokens_count": len(thought.token_chain)
                }
                for thought in list(self.thought_history)[-5:]
            ]
        }
        
        return report

# Globalna instancja procesora
_inner_language_processor = None

def get_inner_language_processor() -> InnerLanguageProcessor:
    """Pobierz globaln instancj procesora jzyka wewntrznego"""
    global _inner_language_processor
    if _inner_language_processor is None:
        _inner_language_processor = InnerLanguageProcessor()
    return _inner_language_processor

async def process_inner_thought(text: str, context: Dict[str, Any] = None) -> InnerThought:
    """
    G贸wna funkcja przetwarzania myli na jzyk wewntrzny
    
    Args:
        text: Tekst w jzyku naturalnym
        context: Kontekst przetwarzania
        
    Returns:
        InnerThought: Przetworzona myl
    """
    processor = get_inner_language_processor()
    return await processor.process_natural_language_input(text, context)

async def translate_inner_thought(
    inner_thought: InnerThought, 
    style: str = "natural"
) -> str:
    """
    Przetumacz myl z jzyka wewntrznego na naturalny
    
    Args:
        inner_thought: Myl w jzyku wewntrznym
        style: Styl tumaczenia
        
    Returns:
        str: Tekst w jzyku naturalnym
    """
    processor = get_inner_language_processor()
    return await processor.translate_to_natural_language(inner_thought, style)

# Test funkcji
if __name__ == "__main__":
    async def test_inner_language():
        """Test systemu jzyka wewntrznego"""
        
        test_inputs = [
            "Jak mog lepiej zrozumie sztuczn inteligencj?",
            "Czy AI mo偶e by kreatywna i czy to oznacza wiadomo?",
            "Interesuje mnie implementacja algorytm贸w uczenia maszynowego w praktyce.",
            "Zastanawiam si nad etyk sztucznej inteligencji i jej wpywem na spoeczestwo."
        ]
        
        print(" TEST JZYKA WEWNTRZNEGO")
        print("=" * 60)
        
        processor = get_inner_language_processor()
        
        # Test przetwarzania input
        thoughts = []
        for i, text in enumerate(test_inputs, 1):
            print(f"\n INPUT {i}: {text}")
            print("-" * 50)
            
            # Przetw贸rz na jzyk wewntrzny
            inner_thought = await process_inner_thought(text)
            thoughts.append(inner_thought)
            
            print(f" Tokeny ({len(inner_thought.token_chain)}): {inner_thought.token_chain[:5]}...")
            print(f" Kompresja: {inner_thought.compression_level:.2f}")
            print(f" Pewno: {inner_thought.confidence:.2f}")
            print(f" Oryginalno: {inner_thought.originality:.2f}")
            print(f"憋 Czas: {inner_thought.processing_time:.3f}s")
            
            # Przetumacz z powrotem
            translation = await translate_inner_thought(inner_thought, "natural")
            print(f" Tumaczenie: {translation[:100]}...")
        
        # Test wykrywania wzorc贸w
        print(f"\n WYKRYWANIE WZORCW MYLOWYCH")
        print("-" * 40)
        
        patterns = await processor.discover_thought_patterns(min_frequency=1)
        print(f"Odkryto wzorc贸w: {len(patterns)}")
        
        for pattern in patterns[:3]:
            print(f"  Wzorzec: {pattern.token_sequence}")
            print(f"  Skuteczno: {pattern.effectiveness_score:.2f}")
            print(f"  Czstotliwo: {pattern.usage_frequency:.2f}")
        
        # Test klastrowania
        print(f"\nЗ KLASTROWANIE SEMANTYCZNE")
        print("-" * 35)
        
        clusters = await processor.cluster_semantic_tokens(similarity_threshold=0.5, min_cluster_size=2)
        print(f"Utworzono klastr贸w: {len(clusters)}")
        
        for cluster in clusters[:3]:
            print(f"  Klaster: {cluster.cluster_theme}")
            print(f"  Tokeny: {len(cluster.core_tokens + cluster.peripheral_tokens)}")
            print(f"  Sp贸jno: {cluster.coherence_score:.2f}")
        
        # Test skojarze
        print(f"\n TEST SKOJARZE")
        print("-" * 25)
        
        if thoughts:
            sample_tokens = thoughts[0].token_chain[:3]
            associations = await processor.find_associations(sample_tokens, max_associations=5)
            
            print(f"Skojarzenia dla: {sample_tokens}")
            for token_id, strength in associations:
                if token_id in processor.token_dictionary:
                    surface = processor.token_dictionary[token_id].surface_form
                    print(f"  {surface}: {strength:.3f}")
        
        # Raport kocowy
        report = await processor.get_inner_language_report()
        print(f"\n RAPORT SYSTEMU:")
        print(f"Token贸w og贸em: {report['token_statistics']['total_tokens']}")
        print(f"Klastr贸w: {report['clustering']['total_clusters']}")
        print(f"Wzorc贸w: {report['patterns']['total_patterns']}")
        print(f"rednia kompresja: {report['performance_metrics']['compression_ratio']:.2f}")
        print(f"Myli przetworzone: {report['performance_metrics']['thoughts_processed']}")
    
    # Uruchom test
    asyncio.run(test_inner_language())