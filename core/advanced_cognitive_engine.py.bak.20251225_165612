"""
ğŸ”— INTEGRACJA SYSTEMÃ“W KOGNITYWNYCH
=================================

Integracja wszystkich 5 zaawansowanych systemÃ³w kognitywnych z gÅ‚Ã³wnym silnikiem.
Implementacja orchestracji, konfiguracji i sterowania przepÅ‚ywem przetwarzania.

Autor: Zaawansowany System Kognitywny MRD
Data: 15 paÅºdziernika 2025
"""

import asyncio
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

from .config import *
from .llm import get_llm_client
from .memory import get_memory_manager
from .hierarchical_memory import get_hierarchical_memory_system
from .helpers import log_info, log_error, log_warning

# Import wszystkich systemÃ³w kognitywnych (opcjonalne - fallback jeÅ›li brak)
try:
    from .self_reflection import get_self_reflection_engine, ReflectionDepth
except ImportError:
    get_self_reflection_engine = None
    ReflectionDepth = None

try:
    from .knowledge_compression import get_knowledge_compressor
except ImportError:
    get_knowledge_compressor = None

try:
    from .multi_agent_orchestrator import get_multi_agent_orchestrator
except ImportError:
    get_multi_agent_orchestrator = None

try:
    from .future_predictor import get_future_predictor, PredictionHorizon
except ImportError:
    get_future_predictor = None
    PredictionHorizon = None

try:
    from .inner_language import get_inner_language_processor
except ImportError:
    get_inner_language_processor = None

class CognitiveMode(Enum):
    """Tryby pracy systemu kognitywnego"""
    BASIC = "basic"                    # Tylko podstawowe przetwarzanie
    ENHANCED = "enhanced"              # Z refleksjÄ… i kompresjÄ…
    ADVANCED = "advanced"              # Ze wszystkimi systemami
    PREDICTIVE = "predictive"          # Z predykcjÄ… przyszÅ‚oÅ›ci
    MULTI_AGENT = "multi_agent"        # Z orkiestracjÄ… agentÃ³w
    FULL_COGNITIVE = "full_cognitive"  # Wszystkie systemy aktywne

class ProcessingStage(Enum):
    """Etapy przetwarzania kognitywnego"""
    INPUT_ANALYSIS = "input_analysis"
    INNER_LANGUAGE = "inner_language"
    MEMORY_SEARCH = "memory_search"
    KNOWLEDGE_COMPRESSION = "knowledge_compression"
    MULTI_AGENT = "multi_agent"
    RESPONSE_GENERATION = "response_generation"
    SELF_REFLECTION = "self_reflection"
    FUTURE_PREDICTION = "future_prediction"
    OUTPUT_SYNTHESIS = "output_synthesis"

@dataclass
class CognitiveResult:
    """Wynik przetwarzania kognitywnego"""
    primary_response: str
    reflection_insights: List[Dict[str, Any]]
    agent_perspectives: List[Dict[str, Any]]
    future_predictions: List[Dict[str, Any]]
    compressed_knowledge: Dict[str, Any]
    inner_thought: Dict[str, Any]
    processing_metrics: Dict[str, float]
    confidence_score: float
    originality_score: float
    total_processing_time: float

class AdvancedCognitiveEngine:
    """
    ğŸ§  Zaawansowany Silnik Kognitywny
    
    Orkiestracja wszystkich 5 systemÃ³w kognitywnych:
    1. Self-Reflection Engine - dynamiczna refleksja
    2. Knowledge Compression - kompresja i transfer learning  
    3. Multi-Agent Orchestrator - wieloagentowe myÅ›lenie
    4. Future Predictor - przewidywanie kontekstu
    5. Inner Language - wewnÄ™trzny jÄ™zyk semantyczny
    """
    
    def __init__(self):
        self.llm_client = get_llm_client()
        self.memory = get_memory_manager()
        self.hierarchical_memory = get_hierarchical_memory_system()
        
        # Inicjalizacja systemÃ³w kognitywnych (z fallbackiem jeÅ›li brak)
        self.self_reflection = get_self_reflection_engine() if get_self_reflection_engine else None
        self.knowledge_compressor = get_knowledge_compressor() if get_knowledge_compressor else None
        self.multi_agent = get_multi_agent_orchestrator() if get_multi_agent_orchestrator else None
        self.future_predictor = get_future_predictor() if get_future_predictor else None
        self.inner_language = get_inner_language_processor() if get_inner_language_processor else None
        
        # Konfiguracja
        self.default_mode = CognitiveMode.ENHANCED
        self.enable_caching = True
        self.parallel_processing = True
        self.adaptive_depth = True
        
        # Metryki
        self.processing_stats = {
            "total_requests": 0,
            "avg_processing_time": 0.0,
            "cache_hit_rate": 0.0,
            "reflection_improvements": 0,
            "prediction_accuracy": 0.0,
            "knowledge_synthesis_count": 0,
            "multi_agent_consensus_rate": 0.0
        }
        
        log_info("[COGNITIVE_ENGINE] Zaawansowany silnik kognitywny zainicjalizowany")
    
    async def process_message(
        self,
        user_message: str,
        user_id: str,
        conversation_context: List[Dict[str, Any]] = None,
        cognitive_mode: CognitiveMode = None,
        enable_prediction: bool = True,
        reflection_depth: ReflectionDepth = None,
        custom_agents: List[str] = None
    ) -> CognitiveResult:
        """
        GÅ‚Ã³wna funkcja przetwarzania wiadomoÅ›ci przez wszystkie systemy kognitywne
        
        Args:
            user_message: WiadomoÅ›Ä‡ uÅ¼ytkownika
            user_id: ID uÅ¼ytkownika
            conversation_context: Kontekst konwersacji
            cognitive_mode: Tryb pracy kognitywnej
            enable_prediction: Czy wÅ‚Ä…czyÄ‡ predykcjÄ™ przyszÅ‚oÅ›ci
            reflection_depth: GÅ‚Ä™bokoÅ›Ä‡ refleksji
            custom_agents: Niestandardowi agenci
            
        Returns:
            CognitiveResult: Kompleksowy wynik przetwarzania
        """
        
        start_time = time.time()
        processing_metrics = {}
        
        try:
            # Ustaw domyÅ›lny tryb
            if cognitive_mode is None:
                cognitive_mode = self.default_mode
            
            log_info(f"[COGNITIVE_ENGINE] Przetwarzanie w trybie: {cognitive_mode.value}")
            
            # ETAP 1: Analiza input i konwersja na jÄ™zyk wewnÄ™trzny
            stage_start = time.time()
            inner_thought = await self._process_inner_language(user_message, conversation_context)
            processing_metrics["inner_language_time"] = time.time() - stage_start
            
            # ETAP 2: Wyszukiwanie w pamiÄ™ci z kompresjÄ… wiedzy
            stage_start = time.time()
            memory_context, compressed_knowledge = await self._enhanced_memory_search(
                user_message, user_id, inner_thought
            )
            processing_metrics["memory_search_time"] = time.time() - stage_start
            
            # ETAP 3: SprawdÅº predykcje z cache (jeÅ›li wÅ‚Ä…czone)
            prediction_hit = None
            if enable_prediction and cognitive_mode in [CognitiveMode.PREDICTIVE, CognitiveMode.FULL_COGNITIVE]:
                prediction_hit = await self.future_predictor.check_prediction_hit(user_id, user_message)
                if prediction_hit:
                    log_info("[COGNITIVE_ENGINE] ğŸ¯ PREDICTION HIT - uÅ¼ywam przygotowanej odpowiedzi")
            
            # ETAP 4: Generacja odpowiedzi (podstawowa lub z cache)
            stage_start = time.time()
            if prediction_hit and prediction_hit.preparation_confidence > 0.7:
                # UÅ¼yj przygotowanej odpowiedzi
                primary_response = prediction_hit.prepared_content
                processing_metrics["response_generation_time"] = 0.01  # Cache hit
            else:
                # Generuj nowÄ… odpowiedÅº
                primary_response = await self._generate_enhanced_response(
                    user_message, memory_context, compressed_knowledge, inner_thought, cognitive_mode
                )
                processing_metrics["response_generation_time"] = time.time() - stage_start
            
            # ETAP 5: Wieloagentowa analiza (jeÅ›li wÅ‚Ä…czona)
            agent_perspectives = []
            if cognitive_mode in [CognitiveMode.MULTI_AGENT, CognitiveMode.FULL_COGNITIVE]:
                stage_start = time.time()
                agent_perspectives = await self._orchestrate_multi_agent_analysis(
                    user_message, primary_response, conversation_context, custom_agents
                )
                processing_metrics["multi_agent_time"] = time.time() - stage_start
            
            # ETAP 6: Self-reflection i poprawa (jeÅ›li wÅ‚Ä…czona)
            reflection_insights = []
            final_response = primary_response
            
            if cognitive_mode in [CognitiveMode.ENHANCED, CognitiveMode.ADVANCED, CognitiveMode.FULL_COGNITIVE]:
                stage_start = time.time()
                
                # Adaptacyjna gÅ‚Ä™bokoÅ›Ä‡ refleksji
                if reflection_depth is None:
                    reflection_depth = await self._determine_reflection_depth(
                        user_message, primary_response, cognitive_mode
                    )
                
                reflection_result = await self.self_reflection.reflect_on_response(
                    original_query=user_message,
                    initial_response=primary_response,
                    context=conversation_context or [],
                    depth=reflection_depth,
                    agent_feedback=agent_perspectives
                )
                
                reflection_insights = reflection_result.get("insights", [])
                improved_response = reflection_result.get("improved_response")
                
                if improved_response and len(improved_response) > len(primary_response) * 0.8:
                    final_response = improved_response
                    self.processing_stats["reflection_improvements"] += 1
                
                processing_metrics["reflection_time"] = time.time() - stage_start
            
            # ETAP 7: Predykcja przyszÅ‚ych zapytaÅ„ (jeÅ›li wÅ‚Ä…czona)
            future_predictions = []
            if enable_prediction and cognitive_mode in [CognitiveMode.PREDICTIVE, CognitiveMode.FULL_COGNITIVE]:
                stage_start = time.time()
                future_predictions = await self._generate_future_predictions(
                    user_id, user_message, conversation_context
                )
                processing_metrics["future_prediction_time"] = time.time() - stage_start
            
            # ETAP 8: Oblicz metryki koÅ„cowe
            confidence_score = await self._calculate_overall_confidence(
                final_response, reflection_insights, agent_perspectives, compressed_knowledge
            )
            
            originality_score = await self._calculate_originality_score(
                inner_thought, compressed_knowledge, agent_perspectives
            )
            
            total_time = time.time() - start_time
            processing_metrics["total_time"] = total_time
            
            # ETAP 9: Aktualizuj statystyki
            await self._update_processing_stats(total_time, confidence_score, len(future_predictions))
            
            # StwÃ³rz wynik
            result = CognitiveResult(
                primary_response=final_response,
                reflection_insights=reflection_insights,
                agent_perspectives=agent_perspectives,
                future_predictions=future_predictions,
                compressed_knowledge=compressed_knowledge,
                inner_thought={
                    "token_chain": inner_thought.token_chain if inner_thought else [],
                    "compression_level": inner_thought.compression_level if inner_thought else 0.0,
                    "confidence": inner_thought.confidence if inner_thought else 0.5,
                    "originality": inner_thought.originality if inner_thought else 0.5
                },
                processing_metrics=processing_metrics,
                confidence_score=confidence_score,
                originality_score=originality_score,
                total_processing_time=total_time
            )
            
            log_info(f"[COGNITIVE_ENGINE] Przetwarzanie zakoÅ„czone: {total_time:.2f}s, confidence: {confidence_score:.2f}")
            return result
            
        except Exception as e:
            log_error(f"[COGNITIVE_ENGINE] BÅ‚Ä…d przetwarzania kognitywnego: {e}")
            return await self._create_fallback_result(user_message)
    
    async def _process_inner_language(
        self, 
        user_message: str, 
        conversation_context: List[Dict[str, Any]] = None
    ):
        """PrzetwÃ³rz wiadomoÅ›Ä‡ na jÄ™zyk wewnÄ™trzny"""
        
        context_data = {
            "conversation_length": len(conversation_context) if conversation_context else 0,
            "recent_topics": [msg.get("content", "")[:100] for msg in (conversation_context or [])[-3:]],
            "message_type": "question" if "?" in user_message else "statement"
        }
        
        # Fallback if inner_language is None
        if self.inner_language:
            return await self.inner_language.process_natural_language_input(user_message, context_data)
        else:
            # Basic fallback processing
            return {
                "analyzed_intent": "general_query",
                "tokens": user_message.split(),
                "sentiment": "neutral",
                "entities": []
            }
            }
        # FIX: search_hybrid nie istnieje, uÅ¼yj ltm_search_hybrid
        from .memory import ltm_search_hybrid
        memory_results = ltm_search_hybrid(user_message, limit=10), Dict[str, Any]]:
        """Rozszerzone wyszukiwanie w pamiÄ™ci z kompresjÄ… wiedzy"""
        
        # Standardowe wyszukiwanie w pamiÄ™ci
        memory_results = await self.hierarchical_memory.search_hybrid(
            query=user_message,
            user_id=user_id,
            max_results=10
        )
        
        # Kompresja i synteza wiedzy
        if len(memory_results) > 3 and self.knowledge_compressor:
            # Przygotuj konwersacje do kompresji
            conversations = []
            for result in memory_results:
                if result.get("conversation_context"):
                    conversations.append({
                        "messages": result["conversation_context"],
                        "timestamp": result.get("timestamp", datetime.now()),
                        "user_id": user_id
                    })
            
            try:
                # Skompresuj wiedzÄ™
                compressed_knowledge = await self.knowledge_compressor.compress_conversations(
                    conversations, user_id
                )
                
                # Synteza nowej wiedzy
                if len(conversations) > 1:
                    synthesized = await self.knowledge_compressor.synthesize_new_knowledge(
                        compressed_knowledge.get("knowledge_vectors", []),
                        user_id,
                        current_topic=user_message[:100]
                    )
                    compressed_knowledge.update(synthesized)
            except Exception as e:
                print(f"[WARN] Knowledge compression failed: {e}")
                compressed_knowledge = {}
        
        else:
            compressed_knowledge = {}
        
        return memory_results, compressed_knowledge
    
    async def _generate_enhanced_response(
        self,
        user_message: str,
        memory_context: List[Dict[str, Any]],
        compressed_knowledge: Dict[str, Any],
        inner_thought,
        cognitive_mode: CognitiveMode
    ) -> str:
        """Generuj ulepszonÄ… odpowiedÅº z peÅ‚nym kontekstem"""
        
        # Przygotuj kontekst dla LLM
        context_elements = []
        
        # Dodaj compressed knowledge
        if compressed_knowledge.get("compressed_themes"):
            context_elements.append(f"Skompresowane tematy: {', '.join(compressed_knowledge['compressed_themes'][:5])}")
        
        if compressed_knowledge.get("thinking_patterns"):
            patterns = compressed_knowledge["thinking_patterns"]
            if patterns:
                context_elements.append(f"Wzorce myÅ›lowe: {patterns[0].get('description', '')}")
        
        # Dodaj inner language insights
        if inner_thought:
            context_elements.append(f"Kompresja myÅ›li: {inner_thought.compression_level:.2f}")
            if inner_thought.confidence > 0.7:
                context_elements.append("Wysoka pewnoÅ›Ä‡ interpretacji")
        
        # Dodaj memory context (skrÃ³cony)
        if memory_context:
            relevant_memories = [mem.get("content", "")[:200] for mem in memory_context[:3]]
            context_elements.append(f"PamiÄ™Ä‡ kontekstowa: {'; '.join(relevant_memories)}")
        
        enhanced_prompt = f"""
        Odpowiedz na zapytanie uÅ¼ytkownika, wykorzystujÄ…c dostÄ™pny kontekst:
        
        ZAPYTANIE: {user_message}
        
        KONTEKST KOGNITYWNY:
        {chr(10).join(f"- {element}" for element in context_elements)}
        
        TRYB PRZETWARZANIA: {cognitive_mode.value}
        
        Wytyczne odpowiedzi:
        1. Wykorzystaj wszystkie dostÄ™pne informacje kontekstowe
        2. Dostosuj szczegÃ³Å‚owoÅ›Ä‡ do trybu kognitywnego
        3. Zachowaj naturalnoÅ›Ä‡ i pÅ‚ynnoÅ›Ä‡ odpowiedzi
        4. WÅ‚Ä…cz insights z analizy kognitywnej gdzie to stosowne
        5. BÄ…dÅº konkretny i praktyczny
        
        OdpowiedÅº:
        """
        
        try:
            response = await self.llm_client.chat_completion([{
                "role": "system",
                "content": f"JesteÅ› zaawansowanym asystentem AI z moÅ¼liwoÅ›ciami kognitywnego przetwarzania w trybie {cognitive_mode.value}. Wykorzystujesz kontekst z pamiÄ™ci, kompresji wiedzy i analizy wewnÄ™trznego jÄ™zyka."
            }, {
                "role": "user",
                "content": enhanced_prompt
            }])
            
            return response
            
        except Exception as e:
            log_error(f"[COGNITIVE_ENGINE] BÅ‚Ä…d generacji odpowiedzi: {e}")
            return f"Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d podczas przetwarzania Twojego zapytania: {user_message}"
    
    async def _orchestrate_multi_agent_analysis(
        self,
        user_message: str,
        primary_response: str,
        conversation_context: List[Dict[str, Any]],
        custom_agents: List[str] = None
    ) -> List[Dict[str, Any]]:
        """Orkiestruj analizÄ™ wieloagentowÄ…"""
        
        try:
            result = await self.multi_agent.orchestrate_multi_agent_response(
                user_query=user_message,
                initial_response=primary_response,
                conversation_context=conversation_context or [],
                custom_agents=custom_agents
            )
            
            return result.get("agent_responses", [])
            
        except Exception as e:
            log_error(f"[COGNITIVE_ENGINE] BÅ‚Ä…d analizy wieloagentowej: {e}")
            return []
    
    async def _determine_reflection_depth(
        self,
        user_message: str,
        primary_response: str,
        cognitive_mode: CognitiveMode
    ) -> ReflectionDepth:
        """OkreÅ›l adaptacyjnÄ… gÅ‚Ä™bokoÅ›Ä‡ refleksji"""
        
        # Bazowa gÅ‚Ä™bokoÅ›Ä‡ wedÅ‚ug trybu
        base_depths = {
            CognitiveMode.BASIC: ReflectionDepth.SURFACE,
            CognitiveMode.ENHANCED: ReflectionDepth.MEDIUM,
            CognitiveMode.ADVANCED: ReflectionDepth.DEEP,
            CognitiveMode.PREDICTIVE: ReflectionDepth.DEEP,
            CognitiveMode.MULTI_AGENT: ReflectionDepth.PROFOUND,
            CognitiveMode.FULL_COGNITIVE: ReflectionDepth.TRANSCENDENT
        }
        
        base_depth = base_depths.get(cognitive_mode, ReflectionDepth.MEDIUM)
        
        # Modyfikatory gÅ‚Ä™bokoÅ›ci
        depth_modifiers = 0
        
        # ZÅ‚oÅ¼onoÅ›Ä‡ zapytania
        if len(user_message) > 100:
            depth_modifiers += 1
        if "?" in user_message:
            depth_modifiers += 1
        if any(word in user_message.lower() for word in ["dlaczego", "jak", "w jaki sposÃ³b", "wyjaÅ›nij"]):
            depth_modifiers += 1
        
        # DÅ‚ugoÅ›Ä‡ odpowiedzi
        if len(primary_response) > 500:
            depth_modifiers += 1
        
        # SÅ‚owa kluczowe wymagajÄ…ce gÅ‚Ä™bokiej refleksji
        deep_keywords = ["etyka", "filozofia", "moralnoÅ›Ä‡", "znaczenie", "sens", "wartoÅ›ci", "przekonania"]
        if any(keyword in user_message.lower() for keyword in deep_keywords):
            depth_modifiers += 2
        
        # Mapuj modyfikatory na poziomy gÅ‚Ä™bokoÅ›ci
        depth_levels = list(ReflectionDepth)
        current_index = depth_levels.index(base_depth)
        
        # ZwiÄ™ksz gÅ‚Ä™bokoÅ›Ä‡ w oparciu o modyfikatory
        new_index = min(current_index + depth_modifiers, len(depth_levels) - 1)
        
        return depth_levels[new_index]
    
    async def _generate_future_predictions(
        self,
        user_id: str,
        user_message: str,
        conversation_context: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Generuj predykcje przyszÅ‚ych zapytaÅ„"""
        
        try:
            # Predykcje natychmiastowe
            immediate_predictions = await self.future_predictor.predict_user_intentions(
                user_id, user_message, conversation_context, PredictionHorizon.IMMEDIATE
            )
            
            # Predykcje krÃ³tkoterminowe
            short_term_predictions = await self.future_predictor.predict_user_intentions(
                user_id, user_message, conversation_context, PredictionHorizon.SHORT_TERM
            )
            
            # Kombinuj i formatuj
            all_predictions = []
            
            for prediction in immediate_predictions[:3]:
                all_predictions.append({
                    "query": prediction.predicted_query,
                    "confidence": prediction.confidence,
                    "horizon": "immediate",
                    "triggers": prediction.context_triggers
                })
            
            for prediction in short_term_predictions[:3]:
                all_predictions.append({
                    "query": prediction.predicted_query,
                    "confidence": prediction.confidence,
                    "horizon": "short_term",
                    "triggers": prediction.context_triggers
                })
            
            return all_predictions
            
        except Exception as e:
            log_error(f"[COGNITIVE_ENGINE] BÅ‚Ä…d predykcji przyszÅ‚oÅ›ci: {e}")
            return []
    
    async def _calculate_overall_confidence(
        self,
        final_response: str,
        reflection_insights: List[Dict[str, Any]],
        agent_perspectives: List[Dict[str, Any]],
        compressed_knowledge: Dict[str, Any]
    ) -> float:
        """Oblicz ogÃ³lnÄ… pewnoÅ›Ä‡ wyniku"""
        
        confidence = 0.0
        
        # Bazowa pewnoÅ›Ä‡ z dÅ‚ugoÅ›ci odpowiedzi
        if len(final_response) > 100:
            confidence += 0.3
        
        # Bonus za refleksje
        if reflection_insights:
            reflection_confidence = sum(
                insight.get("confidence", 0.5) for insight in reflection_insights
            ) / len(reflection_insights)
            confidence += reflection_confidence * 0.3
        
        # Bonus za konsensus agentÃ³w
        if agent_perspectives:
            agent_confidence = sum(
                perspective.get("confidence", 0.5) for perspective in agent_perspectives
            ) / len(agent_perspectives)
            confidence += agent_confidence * 0.2
        
        # Bonus za compressed knowledge
        if compressed_knowledge.get("knowledge_vectors"):
            confidence += 0.2
        
        return min(confidence, 1.0)
    
    async def _calculate_originality_score(
        self,
        inner_thought,
        compressed_knowledge: Dict[str, Any],
        agent_perspectives: List[Dict[str, Any]]
    ) -> float:
        """Oblicz oryginalnoÅ›Ä‡ odpowiedzi"""
        
        originality = 0.0
        
        # OryginalnoÅ›Ä‡ z inner language
        if inner_thought:
            originality += inner_thought.originality * 0.4
        
        # OryginalnoÅ›Ä‡ z syntezy wiedzy
        if compressed_knowledge.get("synthetic_memories"):
            originality += 0.3
        
        # RÃ³Å¼norodnoÅ›Ä‡ perspektyw agentÃ³w
        if len(agent_perspectives) > 3:
            originality += 0.3
        
        return min(originality, 1.0)
    
    async def _update_processing_stats(
        self,
        processing_time: float,
        confidence_score: float,
        predictions_count: int
    ):
        """Aktualizuj statystyki przetwarzania"""
        
        self.processing_stats["total_requests"] += 1
        
        # Åšredni czas przetwarzania
        current_avg = self.processing_stats["avg_processing_time"]
        total_requests = self.processing_stats["total_requests"]
        self.processing_stats["avg_processing_time"] = (
            (current_avg * (total_requests - 1) + processing_time) / total_requests
        )
        
        # Aktualizuj inne metryki
        if predictions_count > 0:
            self.processing_stats["prediction_accuracy"] = (
                self.processing_stats["prediction_accuracy"] * 0.9 + confidence_score * 0.1
            )
    
    async def _create_fallback_result(self, user_message: str) -> CognitiveResult:
        """StwÃ³rz podstawowy wynik w przypadku bÅ‚Ä™du"""
        
        fallback_response = f"Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d podczas zaawansowanego przetwarzania Twojego zapytania. Oto podstawowa odpowiedÅº: {user_message}"
        
        return CognitiveResult(
            primary_response=fallback_response,
            reflection_insights=[],
            agent_perspectives=[],
            future_predictions=[],
            compressed_knowledge={},
            inner_thought={},
            processing_metrics={"total_time": 0.1},
            confidence_score=0.3,
            originality_score=0.1,
            total_processing_time=0.1
        )
    
    async def get_cognitive_status(self) -> Dict[str, Any]:
        """Pobierz status wszystkich systemÃ³w kognitywnych"""
        
        try:
            # Zbierz raporty z wszystkich systemÃ³w
            tasks = [
                self.self_reflection.get_reflection_report(),
                self.knowledge_compressor.get_compression_report(),
                self.multi_agent.get_orchestration_report(),
                self.future_predictor.get_prediction_report(),
                self.inner_language.get_inner_language_report()
            ]
            
            reports = await asyncio.gather(*tasks, return_exceptions=True)
            
            status = {
                "cognitive_engine": {
                    "processing_stats": self.processing_stats,
                    "active_systems": 5,
                    "default_mode": self.default_mode.value
                },
                "self_reflection": reports[0] if not isinstance(reports[0], Exception) else {"error": str(reports[0])},
                "knowledge_compression": reports[1] if not isinstance(reports[1], Exception) else {"error": str(reports[1])},
                "multi_agent": reports[2] if not isinstance(reports[2], Exception) else {"error": str(reports[2])},
                "future_prediction": reports[3] if not isinstance(reports[3], Exception) else {"error": str(reports[3])},
                "inner_language": reports[4] if not isinstance(reports[4], Exception) else {"error": str(reports[4])}
            }
            
            return status
            
        except Exception as e:
            log_error(f"[COGNITIVE_ENGINE] BÅ‚Ä…d pobierania statusu: {e}")
            return {"error": str(e)}

# Globalna instancja silnika
_advanced_cognitive_engine = None

def get_advanced_cognitive_engine() -> AdvancedCognitiveEngine:
    """Pobierz globalnÄ… instancjÄ™ zaawansowanego silnika kognitywnego"""
    global _advanced_cognitive_engine
    if _advanced_cognitive_engine is None:
        _advanced_cognitive_engine = AdvancedCognitiveEngine()
    return _advanced_cognitive_engine

# GÅ‚Ã³wne funkcje API
async def process_with_full_cognition(
    user_message: str,
    user_id: str,
    conversation_context: List[Dict[str, Any]] = None,
    mode: str = "enhanced"
) -> Dict[str, Any]:
    """
    GÅ‚Ã³wna funkcja przetwarzania z peÅ‚nÄ… kognicjÄ…
    
    Args:
        user_message: WiadomoÅ›Ä‡ uÅ¼ytkownika
        user_id: ID uÅ¼ytkownika  
        conversation_context: Kontekst konwersacji
        mode: Tryb kognitywny ("basic", "enhanced", "advanced", "full_cognitive")
        
    Returns:
        Dict: Wynik przetwarzania kognitywnego
    """
    
    engine = get_advanced_cognitive_engine()
    
    try:
        cognitive_mode = CognitiveMode(mode)
    except ValueError:
        cognitive_mode = CognitiveMode.ENHANCED
    
    result = await engine.process_message(
        user_message=user_message,
        user_id=user_id,
        conversation_context=conversation_context,
        cognitive_mode=cognitive_mode
    )
    
    # Konwertuj na dict dla API
    return {
        "response": result.primary_response,
        "reflection_insights": result.reflection_insights,
        "agent_perspectives": result.agent_perspectives,
        "future_predictions": result.future_predictions,
        "compressed_knowledge": result.compressed_knowledge,
        "inner_thought": result.inner_thought,
        "metrics": {
            "processing_time": result.total_processing_time,
            "confidence": result.confidence_score,
            "originality": result.originality_score,
            **result.processing_metrics
        }
    }

# Test funkcji
if __name__ == "__main__":
    async def test_advanced_cognitive_engine():
        """Test zaawansowanego silnika kognitywnego"""
        
        test_queries = [
            "Jak dziaÅ‚a uczenie maszynowe i czy AI moÅ¼e byÄ‡ kreatywna?",
            "Jakie sÄ… etyczne implikacje sztucznej inteligencji?",
            "PomÃ³Å¼ mi zrozumieÄ‡ rÃ³Å¼nice miÄ™dzy sieciami neuronowymi a tradycyjnymi algorytmami.",
            "Co sÄ…dzisz o przyszÅ‚oÅ›ci pracy w dobie automatyzacji?"
        ]
        
        print("ğŸ§  TEST ZAAWANSOWANEGO SILNIKA KOGNITYWNEGO")
        print("=" * 70)
        
        engine = get_advanced_cognitive_engine()
        
        for i, query in enumerate(test_queries, 1):
            print(f"\nğŸ¯ TEST {i}: {query}")
            print("-" * 60)
            
            # Test w rÃ³Å¼nych trybach
            modes = [CognitiveMode.BASIC, CognitiveMode.ENHANCED, CognitiveMode.FULL_COGNITIVE]
            
            for mode in modes:
                print(f"\nğŸ“Š TRYB: {mode.value.upper()}")
                
                result = await engine.process_message(
                    user_message=query,
                    user_id=f"test_user_{i}",
                    cognitive_mode=mode
                )
                
                print(f"â±ï¸  Czas: {result.total_processing_time:.2f}s")
                print(f"ğŸ¯ PewnoÅ›Ä‡: {result.confidence_score:.2f}")
                print(f"âœ¨ OryginalnoÅ›Ä‡: {result.originality_score:.2f}")
                print(f"ğŸ“ OdpowiedÅº: {result.primary_response[:100]}...")
                
                if result.reflection_insights:
                    print(f"ğŸ” Refleksji: {len(result.reflection_insights)}")
                
                if result.agent_perspectives:
                    print(f"ğŸ‘¥ Perspektyw agentÃ³w: {len(result.agent_perspectives)}")
                
                if result.future_predictions:
                    print(f"ğŸ”® Predykcji: {len(result.future_predictions)}")
        
        # Status systemu
        print(f"\nğŸ“Š STATUS SYSTEMU KOGNITYWNEGO")
        print("-" * 40)
        
        status = await engine.get_cognitive_status()
        
        main_stats = status.get("cognitive_engine", {}).get("processing_stats", {})
        print(f"ğŸ“ˆ Zapytania ogÃ³Å‚em: {main_stats.get('total_requests', 0)}")
        print(f"â±ï¸  Åšredni czas: {main_stats.get('avg_processing_time', 0):.2f}s")
        print(f"ğŸ¯ DokÅ‚adnoÅ›Ä‡ predykcji: {main_stats.get('prediction_accuracy', 0):.2f}")
        print(f"ğŸ”§ UlepszeÅ„ refleksyjnych: {main_stats.get('reflection_improvements', 0)}")
    
    # Uruchom test
    asyncio.run(test_advanced_cognitive_engine())