#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Semantic Module - SemanticAnalyzer, SemanticIntegration
FULL LOGIC - 1441 lines - NO PLACEHOLDERS!
"""

import os, re, sys, time, json, uuid, sqlite3, random
from typing import Any, Dict, List, Optional

from .config import DB_PATH, CONTEXT_DICTIONARIES
from .helpers import log_info, log_error, tokenize as _tok, tfidf_vec as _tfidf_vec

class SemanticAnalyzer:
    """Klasa do zaawansowanej analizy semantycznej tekstu"""
    def __init__(self):
        self.sentiment_keywords = {
            "pozytywny": ["dobry", "Å›wietny", "doskonaÅ‚y", "zadowolony", "wspaniaÅ‚y", "super", "fajny", "miÅ‚y", "ciekawy", "lubiÄ™", "podoba", "polecam"],
            "negatywny": ["zÅ‚y", "sÅ‚aby", "kiepski", "niedobry", "rozczarowany", "niezadowolony", "fatalny", "beznadziejny", "okropny", "niestety", "problem"],
            "neutralny": ["normalny", "zwykÅ‚y", "standard", "Å›redni", "przeciÄ™tny", "typowy"]
        }
        
        # SÅ‚ownik dla dokÅ‚adniejszej analizy emocji
        self.emotion_keywords = {
            "radoÅ›Ä‡": ["Å›wietny", "super", "zachwycajÄ…cy", "cieszyÄ‡", "uwielbiÄ‡", "radoÅ›Ä‡", "szczÄ™Å›liwy", "entuzjazm", "zadowolony", "radosny", "wow", "hurra"],
            "smutek": ["smutny", "przykro", "Å¼al", "szkoda", "pÅ‚akaÄ‡", "przygnÄ™biony", "przykry", "smuci", "niestety", "rozczarowany", "porzuciÄ‡", "zrezygnowany"],
            "zÅ‚oÅ›Ä‡": ["wkurzony", "zdenerwowany", "wÅ›ciekÅ‚y", "irytuje", "denerwuje", "zÅ‚y", "zirytowany", "wkurza", "frustracja", "wkurzyÄ‡", "zÅ‚oÅ›ciÄ‡"],
            "strach": ["boi siÄ™", "przeraÅ¼ony", "lÄ™k", "obawy", "obawiam", "strach", "martwi", "zatrwoÅ¼ony", "niepewny", "przestraszony", "obawiam siÄ™"],
            "zaskoczenie": ["wow", "zaskoczony", "zdziwiony", "niesamowity", "zaskakujÄ…cy", "niewiarygodny", "szok", "zdumiewajÄ…cy", "niezwykÅ‚y", "nieprawdopodobny"],
            "zaufanie": ["ufam", "wierzÄ™", "polegam", "pewny", "sprawdzony", "bezpieczny", "wiarygodny", "niezawodny", "godny zaufania"],
            "wstrÄ™t": ["obrzydliwy", "ohydny", "niesmaczny", "odraÅ¼ajÄ…cy", "paskudny", "wstrÄ™tny", "niechÄ™Ä‡", "okropny", "obrzydzenie"],
            "oczekiwanie": ["czekam", "oczekujÄ™", "mam nadziejÄ™", "spodziewaÄ‡ siÄ™", "przewidywaÄ‡", "liczyÄ‡", "powinno", "bÄ™dzie", "chciaÅ‚bym"]
        }
        
        self.intention_indicators = {
            "pytanie": ["?", "czy", "jak", "kiedy", "gdzie", "co", "dlaczego", "ile", "ktÃ³ry", "jakie", "proszÄ™ wyjaÅ›niÄ‡"],
            "proÅ›ba": ["proszÄ™", "czy moÅ¼esz", "czy mÃ³gÅ‚byÅ›", "pomÃ³Å¼", "potrzebujÄ™", "zrÃ³b", "wykonaj", "daj", "pokaÅ¼"],
            "stwierdzenie": ["jest", "sÄ…", "myÅ›lÄ™", "sÄ…dzÄ™", "uwaÅ¼am", "moim zdaniem", "wydaje mi siÄ™", "wiem", "rozumiem"]
        }
        
        # SÅ‚owniki kategorii tematycznych
        self.topic_keywords = {
            "technologia": ["komputer", "laptop", "telefon", "internet", "aplikacja", "program", "software", "hardware", "kod", "programowanie"],
            "biznes": ["firma", "przedsiÄ™biorstwo", "zysk", "marketing", "sprzedaÅ¼", "klient", "produkt", "usÅ‚uga", "rynek", "inwestycja"],
            "podrÃ³Å¼e": ["wakacje", "wycieczka", "hotel", "rezerwacja", "lot", "samolot", "zwiedzanie", "turysta", "przewodnik", "destynacja"],
            "zdrowie": ["lekarz", "choroba", "lekarstwo", "terapia", "Ä‡wiczenia", "dieta", "samopoczucie", "zdrowy", "pacjent", "dolegliwoÅ›ci"],
            "edukacja": ["szkoÅ‚a", "nauka", "studia", "uniwersytet", "kurs", "student", "profesor", "egzamin", "wykÅ‚ad", "wiedza"],
            "rozrywka": ["film", "muzyka", "koncert", "spektakl", "ksiÄ…Å¼ka", "gra", "zabawa", "hobby", "serial", "festiwal"]
        }
        print("Analiza semantyczna - inicjalizacja powiodÅ‚a siÄ™")
        
    def analyze_text(self, text):
        """Kompleksowa analiza semantyczna tekstu"""
        if not text:
            return {}
            
        result = {
            "topics": self.detect_topics(text),
            "sentiment": self.analyze_sentiment(text),
            "emotions": self.analyze_emotions(text),
            "intention": self.detect_intention(text),
            "hidden_intentions": self.detect_hidden_intentions(text),
            "keywords": self.extract_keywords(text),
            "complexity": self.analyze_complexity(text),
            "temporal_context": self.detect_temporal_context(text),
            "entities": self.extract_entities(text)
        }
        return result
        
    def analyze_emotions(self, text):
        """Zaawansowana analiza emocji w tekÅ›cie"""
        if not text:
            return {}
            
        text_lower = text.lower()
        tokens = _tok(text_lower) if hasattr(text_lower, '__len__') else []
        words = text_lower.split()
        
        # Analizuj emocje na podstawie sÅ‚Ã³w kluczowych
        emotion_scores = {emotion: 0.0 for emotion in self.emotion_keywords}
        emotion_matches = {}
        
        # Zaimplementujmy podejÅ›cie z uwzglÄ™dnieniem kontekstu
        # Najpierw podstawowe zliczanie
        for emotion, keywords in self.emotion_keywords.items():
            matches = []
            for word in keywords:
                # Bardziej zaawansowane sprawdzenie niÅ¼ proste text_lower.count()
                if len(word.split()) > 1:  # Dla fraz wielowyrazowych
                    if word in text_lower:
                        matches.append(word)
                        emotion_scores[emotion] += 0.2
                else:  # Dla pojedynczych sÅ‚Ã³w
                    # Dopasowanie form wyrazÃ³w (np. smutek, smutny, smutno)
                    pattern = r"\b" + re.escape(word[:4]) + r"[a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅº]*\b"
                    matches_found = re.findall(pattern, text_lower)
                    if matches_found:
                        matches.extend(matches_found)
                        emotion_scores[emotion] += 0.1 * len(matches_found)
            
            if matches:
                emotion_matches[emotion] = matches
        
        # Analiza wzajemnych wzmocnieÅ„ i osÅ‚abieÅ„ emocji
        if emotion_scores.get("radoÅ›Ä‡", 0) > 0 and emotion_scores.get("smutek", 0) > 0:
            # JeÅ›li wystÄ™puje jednoczeÅ›nie radoÅ›Ä‡ i smutek, sprawdÅºmy negacje
            if any(neg in text_lower for neg in ["nie jest", "nie byÅ‚", "nie sÄ…", "nie czujÄ™"]):
                # Prawdopodobnie negacja pozytywnych emocji
                if "nie" in text_lower and any(pos in text_lower[text_lower.find("nie"):] 
                                            for pos in self.emotion_keywords["radoÅ›Ä‡"]):
                    emotion_scores["radoÅ›Ä‡"] *= 0.3
                    emotion_scores["smutek"] *= 1.5
        
        # UwzglÄ™dnienie znakÃ³w interpunkcyjnych i emotikonÃ³w
        if "!" in text:
            # Wykrzykniki wzmacniajÄ… dominujÄ…ce emocje
            max_emotion = max(emotion_scores, key=emotion_scores.get)
            if max_emotion in ["radoÅ›Ä‡", "zÅ‚oÅ›Ä‡", "zaskoczenie"]:
                emotion_scores[max_emotion] += 0.1 * text.count("!")
        
        # Emotikony i emoji
        happy_emojis = [":)", ":D", "ğŸ˜Š", "ğŸ˜", "ğŸ˜„", "ğŸ‘"]
        sad_emojis = [":(", "ğŸ˜¢", "ğŸ˜­", "ğŸ˜”", "ğŸ‘"]
        angry_emojis = ["ğŸ˜ ", "ğŸ˜¡", "ğŸ‘¿", "ğŸ’¢"]
        surprised_emojis = ["ğŸ˜®", "ğŸ˜¯", "ğŸ˜²", "ğŸ˜±", "ğŸ˜³"]
        
        for emoji in happy_emojis:
            count = text.count(emoji)
            if count > 0:
                emotion_scores["radoÅ›Ä‡"] += 0.15 * count
                
        for emoji in sad_emojis:
            count = text.count(emoji)
            if count > 0:
                emotion_scores["smutek"] += 0.15 * count
                
        for emoji in angry_emojis:
            count = text.count(emoji)
            if count > 0:
                emotion_scores["zÅ‚oÅ›Ä‡"] += 0.15 * count
                
        for emoji in surprised_emojis:
            count = text.count(emoji)
            if count > 0:
                emotion_scores["zaskoczenie"] += 0.15 * count
        
        # Analiza intensywnoÅ›ci na podstawie skÅ‚adni i powtarzajÄ…cych siÄ™ wzorÃ³w
        intensity = 1.0
        if re.search(r"bardzo|niezwykle|ogromnie|niesamowicie|wyjÄ…tkowo", text_lower):
            intensity = 1.5
        elif re.search(r"trochÄ™|lekko|nieco|delikatnie", text_lower):
            intensity = 0.7
            
        # Aplikujemy intensywnoÅ›Ä‡ do wynikÃ³w
        for emotion in emotion_scores:
            emotion_scores[emotion] *= intensity
        
        # Normalizacja wynikÃ³w
        total = sum(emotion_scores.values()) or 1.0
        normalized = {k: round(v/total, 2) for k, v in emotion_scores.items() if v > 0}
        
        # DominujÄ…ce emocje (top 3)
        dominant = sorted(normalized.items(), key=lambda x: x[1], reverse=True)[:3]
        dominant_emotions = {emotion: score for emotion, score in dominant if score > 0.1}
        
        return {
            "dominujÄ…ce": dominant_emotions,
            "wszystkie": normalized,
            "intensywnoÅ›Ä‡": round(intensity, 2),
            "dopasowania": emotion_matches
        }
        
    def detect_topics(self, text):
        """Wykrywa tematy w tekÅ›cie z wagami uÅ¼ywajÄ…c TF-IDF"""
        if not text:
            return {}
            
        text_lower = text.lower()
        text_tokens = _tok(text_lower)  # UÅ¼ywamy istniejÄ…cej funkcji tokenizujÄ…cej
        
        # Przygotowanie korpusu dokumentÃ³w do TF-IDF
        corpus = []
        topic_docs = {}
        
        # Tworzenie dokumentÃ³w dla kaÅ¼dego tematu (dla TF-IDF)
        for topic, keywords in self.topic_keywords.items():
            topic_docs[topic] = " ".join(keywords)
            corpus.append(topic_docs[topic])
        
        # Dodaj zapytanie uÅ¼ytkownika jako ostatni dokument w korpusie
        corpus.append(text_lower)
        
        # Obliczenie wektorÃ³w TF-IDF
        tfidf_scores = _tfidf_vec(text_tokens, [_tok(doc) for doc in corpus])
        
        # Obliczenie podobieÅ„stwa miÄ™dzy tekstem a tematami
        topic_scores = {}
        for topic, topic_text in topic_docs.items():
            topic_tokens = _tok(topic_text)
            topic_tfidf = _tfidf_vec(topic_tokens, [_tok(doc) for doc in corpus])
            
            # Iloczyn skalarny wektorÃ³w TF-IDF (prostszy odpowiednik cosine similarity)
            score = 0
            for term in set(text_tokens) & set(topic_tokens):  # WspÃ³lne terminy
                score += tfidf_scores.get(term, 0) * topic_tfidf.get(term, 0) * 3.0  # Waga dla wspÃ³lnych terminÃ³w
                
            # Dodatkowa korekta dla sÅ‚Ã³w kluczowych
            for keyword in self.topic_keywords[topic]:
                if keyword in text_lower:
                    score += 0.15  # Bonus za dokÅ‚adne dopasowanie sÅ‚Ã³w kluczowych
            
            if score > 0.1:  # Minimalny prÃ³g
                topic_scores[topic] = min(0.95, score)  # Ograniczenie maksymalnej wartoÅ›ci
        
        # Dodatkowa analiza kontekstualna
        # Wzorce zakupowe
        if re.search(r'\b(kup|kupi[Ä‡Ä™cÅ‚]|zam[Ã³o]wi[Ä‡Ä™cÅ‚]|sprzeda[Ä‡Ä™cÅ‚]|cen[ayÄ™]|koszt|ofert[ayÄ™]|tani|drogi)\b', text_lower):
            topic_scores["zakupy"] = max(topic_scores.get("zakupy", 0), 0.7)
            
        # Wzorce wsparcia technicznego
        if re.search(r'\b(problem|trudno[Å›sÄ‡][Ä‡cÄ™]|b[Å‚l][Ä…a]d|nie dzia[Å‚l]a|zepsut|pom[Ã³o][Å¼z])\b', text_lower):
            topic_scores["wsparcie"] = max(topic_scores.get("wsparcie", 0), 0.75)
            
        # Wzorce finansowe
        if re.search(r'\b(pieni[Ä…a]dz|z[Å‚l]ot|pln|eur|usd|walut|bank|konto|p[Å‚l]atno[Å›sÄ‡][Ä‡c])\b', text_lower):
            topic_scores["finanse"] = max(topic_scores.get("finanse", 0), 0.7)
            
        # Normalizacja wynikÃ³w
        total_score = sum(topic_scores.values()) or 1.0
        for topic in topic_scores:
            topic_scores[topic] = topic_scores[topic] / total_score * 0.8 + 0.1  # Skalowanie do sensownego zakresu
            
        # UsuÅ„ tematy z bardzo niskim wynikiem
        return {k: round(v, 2) for k, v in topic_scores.items() if v > 0.22}
    
    def analyze_sentiment(self, text):
        """Analiza sentymentu tekstu"""
        text_lower = text.lower()
        scores = {"pozytywny": 0, "negatywny": 0, "neutralny": 0}
        
        # Liczenie wystÄ…pieÅ„ sÅ‚Ã³w z kaÅ¼dej kategorii
        for sentiment, words in self.sentiment_keywords.items():
            for word in words:
                count = text_lower.count(word)
                if count > 0:
                    scores[sentiment] += count * 0.1  # KaÅ¼de wystÄ…pienie zwiÄ™ksza wynik
        
        # Analiza znakÃ³w interpunkcyjnych i emoji
        if "!" in text:
            excl_count = text.count("!")
            if scores["pozytywny"] > scores["negatywny"]:
                scores["pozytywny"] += excl_count * 0.05
            elif scores["negatywny"] > scores["pozytywny"]:
                scores["negatywny"] += excl_count * 0.05
                
        # SprawdÅº emoji lub emotikony
        positive_emotes = [":)", ":D", "ğŸ˜Š", "ğŸ‘", "ğŸ˜"]
        negative_emotes = [":(", ":(", "ğŸ˜¢", "ğŸ‘", "ğŸ˜ "]
        
        for emote in positive_emotes:
            scores["pozytywny"] += text.count(emote) * 0.15
            
        for emote in negative_emotes:
            scores["negatywny"] += text.count(emote) * 0.15
        
        # SprawdÅº negacjÄ™, ktÃ³ra moÅ¼e odwracaÄ‡ sentyment
        negation_words = ["nie", "bez", "nigdy", "Å¼aden"]
        for word in negation_words:
            pattern = word + " [\\w]+ "
            matches = re.findall(pattern, text_lower)
            if matches:
                # Zmniejsz wpÅ‚yw pozytywnych sÅ‚Ã³w po negacji
                scores["pozytywny"] *= 0.8
                scores["negatywny"] *= 1.2
                
        # Normalizacja wynikÃ³w
        total = sum(scores.values()) or 1
        normalized = {k: round(v/total, 2) for k, v in scores.items()}
        
        # OkreÅ›lenie dominujÄ…cego sentymentu
        dominant = max(normalized, key=normalized.get)
        normalized["dominujÄ…cy"] = dominant
        
        return normalized
        
    def detect_intention(self, text):
        """Wykrywanie intencji uÅ¼ytkownika"""
        text_lower = text.lower()
        scores = {"pytanie": 0, "proÅ›ba": 0, "stwierdzenie": 0}
        
        # SprawdÅº znaki zapytania
        if "?" in text:
            scores["pytanie"] += 0.6
        
        # Sprawdzanie wskaÅºnikÃ³w intencji
        for intention, indicators in self.intention_indicators.items():
            for indicator in indicators:
                if indicator in text_lower:
                    scores[intention] += 0.15
        
        # Analiza struktury gramatycznej (podstawowa)
        if text_lower.startswith("czy") or text_lower.startswith("jak") or text_lower.startswith("kiedy"):
            scores["pytanie"] += 0.3
            
        if "proszÄ™" in text_lower or "czy moÅ¼esz" in text_lower or text_lower.startswith("pomÃ³Å¼"):
            scores["proÅ›ba"] += 0.3
            
        if "." in text and "?" not in text:
            scores["stwierdzenie"] += 0.2
            
        # Normalizacja wynikÃ³w
        total = sum(scores.values()) or 1
        normalized = {k: round(v/total, 2) for k, v in scores.items()}
        
        # OkreÅ›lenie dominujÄ…cej intencji
        dominant = max(normalized, key=normalized.get)
        normalized["dominujÄ…ca"] = dominant
        
        return normalized
    
    def extract_keywords(self, text):
        """Ekstrakcja sÅ‚Ã³w kluczowych z tekstu"""
        # Proste czyszczenie tekstu
        text_lower = re.sub(r'[^\w\s]', ' ', text.lower())
        words = text_lower.split()
        
        # Lista stop words (podstawowa)
        stop_words = ["i", "w", "na", "z", "do", "od", "dla", "Å¼e", "to", "jest", "sÄ…", "byÄ‡", "a", "o", "jak", "tak", "nie", "siÄ™"]
        
        # Filtrowanie sÅ‚Ã³w
        filtered_words = [word for word in words if word not in stop_words and len(word) > 2]
        
        # Obliczanie czÄ™stoÅ›ci wystÄ™powania
        word_freq = {}
        for word in filtered_words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # Sortowanie po czÄ™stoÅ›ci i zwracanie top N sÅ‚Ã³w
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        top_keywords = [word for word, freq in sorted_words[:10]]
        
        return top_keywords
    
    def analyze_complexity(self, text):
        """Analiza zÅ‚oÅ¼onoÅ›ci tekstu"""
        if not text:
            return {"poziom": "brak tekstu", "Å›rednia_dÅ‚ugoÅ›Ä‡_zdania": 0, "Å›rednia_dÅ‚ugoÅ›Ä‡_sÅ‚owa": 0, "rÃ³Å¼norodnoÅ›Ä‡_leksykalna": 0}
            
        # PodziaÅ‚ na zdania
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            return {"poziom": "brak tekstu", "Å›rednia_dÅ‚ugoÅ›Ä‡_zdania": 0, "Å›rednia_dÅ‚ugoÅ›Ä‡_sÅ‚owa": 0, "rÃ³Å¼norodnoÅ›Ä‡_leksykalna": 0}
        
        # Liczba sÅ‚Ã³w w zdaniach
        words_per_sentence = [len(s.split()) for s in sentences]
        avg_sentence_length = sum(words_per_sentence) / len(sentences) if sentences else 0
        
        # Åšrednia dÅ‚ugoÅ›Ä‡ sÅ‚owa
        all_words = [word for s in sentences for word in s.split()]
        if not all_words:
            return {"poziom": "brak tekstu", "Å›rednia_dÅ‚ugoÅ›Ä‡_zdania": 0, "Å›rednia_dÅ‚ugoÅ›Ä‡_sÅ‚owa": 0, "rÃ³Å¼norodnoÅ›Ä‡_leksykalna": 0}
            
        avg_word_length = sum(len(word) for word in all_words) / len(all_words)
        
        # RÃ³Å¼norodnoÅ›Ä‡ leksykalna (unique words / total words)
        lexical_diversity = len(set(all_words)) / len(all_words) if all_words else 0
        
        # OkreÅ›lenie poziomu zÅ‚oÅ¼onoÅ›ci
        complexity_level = "niska"
        if avg_sentence_length > 15 or avg_word_length > 6 or lexical_diversity > 0.7:
            complexity_level = "wysoka"
        elif avg_sentence_length > 10 or avg_word_length > 5 or lexical_diversity > 0.5:
            complexity_level = "Å›rednia"
            
        return {
            "poziom": complexity_level,
            "Å›rednia_dÅ‚ugoÅ›Ä‡_zdania": round(avg_sentence_length, 2),
            "Å›rednia_dÅ‚ugoÅ›Ä‡_sÅ‚owa": round(avg_word_length, 2),
            "rÃ³Å¼norodnoÅ›Ä‡_leksykalna": round(lexical_diversity, 2)
        }
        
    def analyze_local_context(self, text):
        """Analizuje lokalny kontekst w tekÅ›cie - lokalizacje, czas, odniesienia"""
        if not text:
            return {"lokalizacje": [], "czas": [], "odniesienia_przestrzenne": [], 
                    "odniesienia_czasowe": []}
            
        text_lower = text.lower()
        
        # SÅ‚owniki do rozpoznawania rodzajÃ³w kontekstu
        # Lokalizacje (miasta, kraje, regiony)
        location_patterns = [
            # "w Warszawie", "do Polski"
            r"\b(?:w|do|z)\s+([A-Z][a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]{2,})\b",
            # Nazwy wÅ‚asne (miasta, kraje)  
            r"\b([A-Z][a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]{2,})\b",
            # Nazwy ulic
            r"\b(?:ulica|ulicy|ul\.|aleja|alei|al\.)\s+([A-Z][a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]+)\b"
        ]
        
        # WyraÅ¼enia czasowe
        time_patterns = [
            r"\b(\d{1,2}:\d{2})\b",  # Format godziny 12:30
            r"\b(\d{1,2})[:\.-]\s?(\d{2})\b",  # Format godziny z separatorem
            r"\bo\s+(?:godz(?:inie)?)\s+(\d{1,2})\b",  # "o godzinie 5"
            r"\b(?:rano|po\s+poÅ‚udniu|wieczorem|w\s+nocy)\b"  # Pory dnia
        ]
        
        # Odniesienia przestrzenne
        spatial_references = [
            r"\b(?:na\s+prawo|na\s+lewo|nad|pod|obok|przy|przed|za|naprzeciw)\b",
            r"\b(?:w\s+pobliÅ¼u|niedaleko|blisko)\b",
            r"\b(?:na\s+pÃ³Å‚noc|na\s+poÅ‚udnie|na\s+wschÃ³d|na\s+zachÃ³d)\b",
            r"\b(?:w\s+centrum|na\s+obrzeÅ¼ach|na\s+peryferiach|w\s+Å›rodku)\b"
        ]
        
        # Odniesienia czasowe
        temporal_references = [
            r"\b(?:wczoraj|dzisiaj|jutro|pojutrze|za\s+tydzieÅ„)\b",
            r"\b(?:w\s+przyszÅ‚ym\s+tygodniu|w\s+przyszÅ‚ym\s+miesiÄ…cu)\b",
            r"\b(?:rano|po\s+poÅ‚udniu|wieczorem|w\s+nocy|o\s+Å›wicie|o\s+zmierzchu)\b",
            r"\b(?:w\s+poniedziaÅ‚ek|we\s+wtorek|w\s+Å›rodÄ™|w\s+czwartek)\b",
            r"\b(?:w\s+piÄ…tek|w\s+sobotÄ™|w\s+niedzielÄ™)\b",
            r"\b(\d{1,2})\s+(?:stycznia|lutego|marca|kwietnia|maja|czerwca)\b",
            r"\b(\d{1,2})\s+(?:lipca|sierpnia|wrzeÅ›nia|paÅºdziernika|listopada|grudnia)\b"
        ]
        
        # Rozpoznawanie lokalizacji
        locations = []
        for pattern in location_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                locations.extend([m[0] if isinstance(m, tuple) else m for m in matches])
        
        # Rozpoznawanie wyraÅ¼eÅ„ czasowych
        times = []
        for pattern in time_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                times.extend([m[0] if isinstance(m, tuple) else m for m in matches])
        
        # Rozpoznawanie odniesieÅ„ przestrzennych
        spatial_refs = []
        for pattern in spatial_references:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                spatial_refs.extend(matches)
        
        # Rozpoznawanie odniesieÅ„ czasowych
        temporal_refs = []
        for pattern in temporal_references:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                if isinstance(matches[0], tuple):
                    # Dla dat w formacie "15 stycznia 2023"
                    temporal_refs.extend([' '.join(filter(None, m)) for m in matches])
                else:
                    temporal_refs.extend(matches)
        
        # Dodatkowe przetwarzanie dla lokalizacji z przyimkami
        processed_locations = []
        for loc in locations:
            # CzyÅ›cimy z przyimkÃ³w i dodatkowych znakÃ³w
            cleaned_loc = re.sub(r'^(?:w|do|z|na|przy)\s+', '', loc)
            cleaned_loc = re.sub(r'[,.:;"\'()]', '', cleaned_loc)
            if len(cleaned_loc) > 2:  # Minimalna dÅ‚ugoÅ›Ä‡ nazwy lokalizacji
                processed_locations.append(cleaned_loc)
        
        # Deduplikacja wynikÃ³w
        locations = list(set(processed_locations))
        times = list(set(times))
        spatial_refs = list(set(spatial_refs))
        temporal_refs = list(set(temporal_refs))
        
        # Sortowanie wynikÃ³w wedÅ‚ug dÅ‚ugoÅ›ci (dÅ‚uÅ¼sze nazwy sÄ… czÄ™sto bardziej specyficzne)
        locations.sort(key=len, reverse=True)
        
        # Usuwanie faÅ‚szywych trafieÅ„ (typowe sÅ‚owa, ktÃ³re nie sÄ… lokalizacjami)
        common_words = ["jako", "tego", "tych", "inne", "moje", "twoje", "nasze"]
        locations = [loc for loc in locations if loc.lower() not in common_words]
        
        # Identyfikacja gÅ‚Ã³wnego kontekstu przestrzenno-czasowego
        main_location = locations[0] if locations else None
        main_time = temporal_refs[0] if temporal_refs else None
        
        return {
            "lokalizacje": locations,
            "czas": times,
            "odniesienia_przestrzenne": spatial_refs,
            "odniesienia_czasowe": temporal_refs,
            "gÅ‚Ã³wna_lokalizacja": main_location,
            "gÅ‚Ã³wny_czas": main_time
        }
    def analyze_discourse(self, text):
        """Analizuje dyskurs - identyfikuje typ, strukturÄ™ i cechy komunikacji"""
        if not text:
            return {"typ_dyskursu": "brak tekstu", "cechy": [], "sÅ‚owa_kluczowe": []}
            
        text_lower = text.lower()
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        if not sentences:
            return {"typ_dyskursu": "brak tekstu", "cechy": [], "sÅ‚owa_kluczowe": []}
            
        # SÅ‚owniki do identyfikacji typÃ³w dyskursu
        discourse_markers = {
            "naukowy": [
                r"\b(?:badania|badanie|analiza|analizy|hipoteza|teoria|wyniki)\b",
                r"\b(?:dowÃ³d|dowody|metodologia|eksperyment|dane|wniosek)\b",
                r"\b(?:wedÅ‚ug\s+(?:\w+\s+){0,2}et\s+al\.|cytujÄ…c|zgodnie\s+z)\b"
            ],
            "polityczny": [
                r"\b(?:paÅ„stwo|wÅ‚adza|rzÄ…d|ustawa|prawo|spoÅ‚eczeÅ„stwo)\b",
                r"\b(?:polityka|polityczny|partia|demokracja|wybory)\b",
                r"\b(?:obywatel|obywatele|obywatelski|konstytucja|wolnoÅ›Ä‡)\b"
            ],
            "biznesowy": [
                r"\b(?:firma|biznes|przedsiÄ™biorstwo|klient|klienci|zysk)\b",
                r"\b(?:sprzedaÅ¼|rynek|marketing|strategia|budÅ¼et|przychÃ³d)\b",
                r"\b(?:produkt|usÅ‚uga|wartoÅ›Ä‡|cena|oferta|umowa|kontrakt)\b"
            ],
            "potoczny": [
                r"\b(?:super|fajnie|ekstra|spoko|ziom|hej|czeÅ›Ä‡|siema|nara)\b",
                r"\b(?:mega|totalnie|generalnie|jakby|wiesz|no\s+wiesz)\b",
                r"(?:!{2,}|\\?{2,})"
            ],
            "perswazyjny": [
                r"\b(?:musisz|powinieneÅ›|naleÅ¼y|trzeba|koniecznie)\b",
                r"\b(?:najlepszy|jedyny|wyjÄ…tkowy|niesamowity|rewolucyjny)\b",
                r"\b(?:przekonaj\s+siÄ™|sprawdÅº|nie\s+przegap|juÅ¼\s+dziÅ›)\b"
            ],
            "emocjonalny": [
                r"\b(?:kocham|nienawidzÄ™|uwielbiam|bojÄ™\s+siÄ™|tÄ™skniÄ™)\b",
                r"\b(?:radoÅ›Ä‡|smutek|zÅ‚oÅ›Ä‡|strach|niepokÃ³j|wzruszenie)\b",
                r"(?:!{2,}|\\?!|\\.{3,})"
            ],
            "informacyjny": [
                r"\b(?:informacja|informujÄ™|zawiadamiam|komunikat|ogÅ‚oszenie)\b",
                r"\b(?:przekazujÄ™|uprzejmie\s+informujÄ™|podajÄ™\s+do\s+wiadomoÅ›ci)\b",
                r"\b(?:dane|fakty|statystyki|zestawienie|podsumowanie)\b"
            ]
        }
        
        # Cechy dyskursu
        discourse_features = {
            "formalny": [
                r"\b(?:szanowny|uprzejmie|z\s+powaÅ¼aniem|niniejszym)\b",
                r"\b(?:pragnÄ™\s+podkreÅ›liÄ‡|naleÅ¼y\s+zaznaczyÄ‡)\b"
            ],
            "nieformalny": [
                r"\b(?:hej|czeÅ›Ä‡|siema|sÅ‚uchaj|wiesz\s+co|no\s+dobra|ok)\b",
                r"(?:!{2,}|\\?{2,})"
            ],
            "argumentacyjny": [
                r"\b(?:poniewaÅ¼|dlatego|zatem|wiÄ™c|skutkiem)\b",
                r"\b(?:po\s+pierwsze|po\s+drugie|z\s+jednej\s+strony)\b",
                r"\b(?:argumentujÄ™|twierdzÄ™|uwaÅ¼am|wnioskujÄ™)\b"
            ],
            "narracyjny": [
                r"\b(?:pewnego\s+dnia|dawno\s+temu|na\s+poczÄ…tku)\b",
                r"\b(?:nastÄ™pnie|po\s+chwili|tymczasem|w\s+koÅ„cu)\b"
            ],
            "dialogowy": [
                r"\b(?:pytam|odpowiadam|mÃ³wiÄ™|twierdzisz|sugerujesz)\b",
                r'''["â€"''].*?["â€"']''',
                r"\b(?:rozmowa|dialog|dyskusja|debata)\b"
            ],
            "opisowy": [
                r"\b(?:jest|byÅ‚|znajdowaÅ‚\s+siÄ™|wyglÄ…daÅ‚|przypominaÅ‚)\b",
                r"\b(?:wysoki|szeroki|ciemny|jasny|czerwony|duÅ¼y)\b"
            ],
            "instruktaÅ¼owy": [
                r"\b(?:najpierw|nastÄ™pnie|potem|na\s+koniec|krok)\b",
                r"\b(?:wÅ‚Ä…cz|wyÅ‚Ä…cz|naciÅ›nij|kliknij|otwÃ³rz|zamknij)\b",
                r"(?:^\s*\d+\.|^\s*-|\*\s)"
            ]
        }
        
        # Analiza typu dyskursu
        discourse_scores = {}
        for disc_type, patterns in discourse_markers.items():
            score = 0
            for pattern in patterns:
                matches = re.findall(pattern, text_lower, re.MULTILINE)
                score += len(matches)
            discourse_scores[disc_type] = score
            
        # Analiza cech dyskursu
        features = []
        for feature, patterns in discourse_features.items():
            score = 0
            for pattern in patterns:
                matches = re.findall(pattern, text_lower, re.MULTILINE)
                score += len(matches)
            if score >= 2:  # PrÃ³g minimalny dla uznania cechy
                features.append(feature)
                
        # Struktura dyskursu - analiza poÅ‚Ä…czeÅ„ logicznych
        logical_connectors = [
            r"\b(?:poniewaÅ¼|bo|gdyÅ¼|dlatego|wiÄ™c|zatem|stÄ…d)\b",
            r"\b(?:jeÅ›li|jeÅ¼eli|o\s+ile|pod\s+warunkiem)\b",
            r"\b(?:ale|lecz|jednak|niemniej|natomiast)\b",
            r"\b(?:po\s+pierwsze|po\s+drugie|przede\s+wszystkim)\b"
        ]
        
        connectors_count = 0
        for pattern in logical_connectors:
            connectors_count += len(re.findall(pattern, text_lower))
            
        # GÄ™stoÅ›Ä‡ logiczna - liczba poÅ‚Ä…czeÅ„ logicznych na zdanie
        logical_density = connectors_count / len(sentences) if sentences else 0
        
        # KompleksowoÅ›Ä‡ dyskursu - Å›rednia dÅ‚ugoÅ›Ä‡ zdania
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0
        complexity_words = re.findall(r'\b\w{10,}\b', text_lower)
        lexical_complexity = len(complexity_words) / len(sentences) if sentences else 0
        
        # OkreÅ›lenie gÅ‚Ã³wnego typu dyskursu
        main_discourse_type = max(discourse_scores.items(), key=lambda x: x[1])[0] \
            if any(score > 0 for score in discourse_scores.values()) else "nieokreÅ›lony"
            
        # SÅ‚owa kluczowe w dyskursie
        words = re.findall(r'\b\w+\b', text_lower)
        word_freq = {}
        for word in words:
            if len(word) > 3:  # Pomijamy krÃ³tkie sÅ‚owa
                word_freq[word] = word_freq.get(word, 0) + 1
                
        # Lista polskich stopwords (sÅ‚Ã³w nieistotnych)
        stopwords = [
            "oraz", "jako", "tylko", "tego", "przez", "jest", "jestem", 
            "jesteÅ›my", "poniewaÅ¼", "Å¼eby", "ktÃ³ry", "ktÃ³ra", "ktÃ³re", 
            "takÅ¼e", "rÃ³wnieÅ¼", "dlatego", "wiÄ™c", "czyli", "gdyÅ¼", "albo",
            "czyli", "lecz", "gdyÅ¼", "oraz", "jednak", "choÄ‡"
        ]
        
        # Filtrowanie sÅ‚Ã³w nieistotnych
        for word in stopwords:
            if word in word_freq:
                del word_freq[word]
                
        # Wybieranie najczÄ™stszych sÅ‚Ã³w jako sÅ‚owa kluczowe
        keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        keywords = [word for word, freq in keywords]
        
        result = {
            "typ_dyskursu": main_discourse_type,
            "cechy": features,
            "sÅ‚owa_kluczowe": keywords,
            "gÄ™stoÅ›Ä‡_logiczna": round(logical_density, 2),
            "zÅ‚oÅ¼onoÅ›Ä‡_leksykalna": round(lexical_complexity, 2),
            "Å›rednia_dÅ‚ugoÅ›Ä‡_zdania": round(avg_sentence_length, 2)
        }
        
        # Dodanie oceny jakoÅ›ci dyskursu
        if logical_density > 0.5 and lexical_complexity > 0.3 and avg_sentence_length > 15:
            result["ocena_jakoÅ›ci"] = "zaawansowany"
        elif logical_density > 0.3 and avg_sentence_length > 10:
            result["ocena_jakoÅ›ci"] = "standardowy"
        else:
            result["ocena_jakoÅ›ci"] = "prosty"
            
        return result
        
    def analyze_arguments(self, text):
        """Analizuje strukturÄ™ argumentacyjnÄ… tekstu"""
        if not text:
            return {"struktura": "brak tekstu", "elementy": [], "jakoÅ›Ä‡": "brak"}
            
        # Dzielimy tekst na zdania
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        if not sentences:
            return {"struktura": "brak tekstu", "elementy": [], "jakoÅ›Ä‡": "brak"}
        
        # Wzorce dla rozpoznawania elementÃ³w argumentacji
        argument_patterns = {
            "teza_gÅ‚Ã³wna": [
                r"\b(?:uwaÅ¼am,\s+Å¼e|twierdzÄ™,\s+Å¼e|moim\s+zdaniem)\b",
                r"\b(?:chciaÅ‚[a]?bym\s+dowieÅ›Ä‡|zamierzam\s+pokazaÄ‡)\b",
                r"\b(?:gÅ‚Ã³wn[ym|Ä…]\s+(?:tez[Ä…|e]|kwesti[Ä…|e])\s+jest)\b"
            ],
            "przesÅ‚anka": [
                r"\b(?:poniewaÅ¼|gdyÅ¼|bowiem|dlatego\s+Å¼e|z\s+powodu)\b",
                r"\b(?:pierwszym\s+argumentem|drugim\s+argumentem)\b",
                r"\b(?:dowodzi\s+tego|Å›wiadczy\s+o\s+tym|potwierdza\s+to)\b"
            ],
            "kontrargument": [
                r"\b(?:jednak|niemniej\s+jednak|z\s+drugiej\s+strony)\b",
                r"\b(?:moÅ¼na\s+(?:by|teÅ¼)\s+(?:zauwaÅ¼yÄ‡|argumentowaÄ‡))\b",
                r"\b(?:przeciwnicy\s+twierdzÄ…|krytycy\s+wskazujÄ…)\b"
            ],
            "konkluzja": [
                r"\b(?:w\s+(?:konsekwencji|rezultacie|efekcie))\b",
                r"\b(?:(?:podsumowujÄ…c|reasumujÄ…c|konkludujÄ…c))\b",
                r"\b(?:(?:ostatecznie|finalnie|w\s+konkluzji))\b"
            ],
            "przykÅ‚ad": [
                r"\b(?:na\s+przykÅ‚ad|przykÅ‚adem\s+jest|dla\s+przykÅ‚adu)\b",
                r"\b(?:doskonale\s+ilustruje\s+to|Å›wiadczy\s+o\s+tym)\b",
                r"\b(?:warto\s+(?:przytoczyÄ‡|wskazaÄ‡)\s+przykÅ‚ad)\b"
            ],
            "definicja": [
                r"\b(?:definiujÄ™|rozumiem\s+(?:przez|jako)|oznacza\s+to)\b",
                r"\b(?:termin|pojÄ™cie)\s+(?:\w+)\s+(?:odnosi\s+siÄ™|oznacza)\b",
                r"(?:(?:^|[.!?]\s+)(?:[A-Z]\w+)\s+(?:to|jest|oznacza))\b"
            ]
        }
        
        # SpÃ³jniki logiczne i ich kategorie
        logical_connectors = {
            "przyczynowe": [
                r"\b(?:poniewaÅ¼|gdyÅ¼|bowiem|dlatego\s+Å¼e|z\s+powodu)\b",
                r"\b(?:w\s+zwiÄ…zku\s+z\s+tym|skutkiem\s+tego)\b"
            ],
            "kontrastujÄ…ce": [
                r"\b(?:jednak|niemniej|natomiast|ale|lecz|choÄ‡|chociaÅ¼)\b",
                r"\b(?:z\s+drugiej\s+strony|przeciwnie|wbrew\s+temu)\b"
            ],
            "wynikowe": [
                r"\b(?:w\s+rezultacie|w\s+efekcie|w\s+konsekwencji)\b",
                r"\b(?:zatem|wiÄ™c|tak\s+wiÄ™c|stÄ…d|dlatego)\b"
            ],
            "wzmacniajÄ…ce": [
                r"\b(?:co\s+wiÄ™cej|ponadto|dodatkowo|w\s+dodatku)\b",
                r"\b(?:nie\s+tylko|rÃ³wnieÅ¼|takÅ¼e|zarÃ³wno)\b"
            ],
            "porzÄ…dkujÄ…ce": [
                r"\b(?:po\s+pierwsze|po\s+drugie|nastÄ™pnie|w\s+koÅ„cu)\b",
                r"\b(?:przede\s+wszystkim|w\s+szczegÃ³lnoÅ›ci|gÅ‚Ã³wnie)\b"
            ]
        }
        
        # Identyfikacja elementÃ³w argumentacji w zdaniach
        argument_structure = {}
        for arg_type, patterns in argument_patterns.items():
            argument_structure[arg_type] = []
            for pattern in patterns:
                for i, sentence in enumerate(sentences):
                    if re.search(pattern, sentence, re.IGNORECASE):
                        argument_structure[arg_type].append({
                            "zdanie": sentence,
                            "pozycja": i + 1
                        })
        
        # Identyfikacja spÃ³jnikÃ³w logicznych
        connectors_found = {}
        for conn_type, patterns in logical_connectors.items():
            connectors_found[conn_type] = 0
            for pattern in patterns:
                for sentence in sentences:
                    matches = re.findall(pattern, sentence, re.IGNORECASE)
                    connectors_found[conn_type] += len(matches)
        
        # OkreÅ›lanie struktury argumentacyjnej
        structure_type = "nieokreÅ›lona"
        elements_found = []
        
        # Sprawdzanie kompletnoÅ›ci argumentacji
        if argument_structure["teza_gÅ‚Ã³wna"] and argument_structure["konkluzja"]:
            if argument_structure["przesÅ‚anka"]:
                if argument_structure["kontrargument"]:
                    structure_type = "zÅ‚oÅ¼ona dialektyczna"
                    elements_found = ["teza", "przesÅ‚anki", "kontrargumenty", "konkluzja"]
                else:
                    structure_type = "prosta liniowa"
                    elements_found = ["teza", "przesÅ‚anki", "konkluzja"]
            else:
                structure_type = "niekompletna"
                elements_found = ["teza", "konkluzja"]
        elif argument_structure["przesÅ‚anka"]:
            if argument_structure["teza_gÅ‚Ã³wna"]:
                structure_type = "niedokoÅ„czona"
                elements_found = ["teza", "przesÅ‚anki"]
            elif argument_structure["konkluzja"]:
                structure_type = "indukcyjna"
                elements_found = ["przesÅ‚anki", "konkluzja"]
            else:
                structure_type = "fragmentaryczna"
                elements_found = ["przesÅ‚anki"]
        elif argument_structure["teza_gÅ‚Ã³wna"]:
            structure_type = "deklaratywna"
            elements_found = ["teza"]
        
        # OkreÅ›lanie jakoÅ›ci argumentacji
        arg_quality = "niska"
        
        # Liczenie elementÃ³w argumentacji
        total_elements = sum(len(items) for items in argument_structure.values())
        
        # Sprawdzanie obecnoÅ›ci definicji i przykÅ‚adÃ³w
        has_definitions = len(argument_structure["definicja"]) > 0
        has_examples = len(argument_structure["przykÅ‚ad"]) > 0
        
        # Liczenie spÃ³jnikÃ³w logicznych
        total_connectors = sum(connectors_found.values())
        
        # Ocena jakoÅ›ci argumentacji
        conn_per_sentence = total_connectors / len(sentences) if sentences else 0
        
        # ZrÃ³Å¼nicowanie typÃ³w spÃ³jnikÃ³w
        connector_diversity = sum(1 for count in connectors_found.values() if count > 0)
        
        # Kryteria jakoÅ›ci
        if (structure_type in ["zÅ‚oÅ¼ona dialektyczna", "prosta liniowa"] and 
                has_definitions and has_examples and conn_per_sentence >= 0.5 and
                connector_diversity >= 3):
            arg_quality = "wysoka"
        elif (total_elements >= 5 and conn_per_sentence >= 0.3 and
              (has_definitions or has_examples) and connector_diversity >= 2):
            arg_quality = "Å›rednia"
        
        # Identyfikacja gÅ‚Ã³wnych argumentÃ³w
        main_args = []
        for arg_type in ["teza_gÅ‚Ã³wna", "przesÅ‚anka", "konkluzja"]:
            for item in argument_structure[arg_type]:
                if item not in main_args:
                    main_args.append(item["zdanie"])
        
        result = {
            "struktura": structure_type,
            "elementy": elements_found,
            "gÅ‚Ã³wne_argumenty": main_args[:3],  # Ograniczamy do 3 najwaÅ¼niejszych
            "jakoÅ›Ä‡": arg_quality,
            "spÃ³jniki_logiczne": {
                "liczba": total_connectors,
                "na_zdanie": round(conn_per_sentence, 2),
                "rodzaje": {k: v for k, v in connectors_found.items() if v > 0}
            }
        }
        
        # Dodajemy ocenÄ™ balansu argumentacji
        if argument_structure["kontrargument"]:
            contra_to_pro_ratio = (len(argument_structure["kontrargument"]) / 
                                 len(argument_structure["przesÅ‚anka"]) 
                                 if argument_structure["przesÅ‚anka"] else 0)
            result["balans_argumentacji"] = round(contra_to_pro_ratio, 2)
            
            if 0.3 <= contra_to_pro_ratio <= 0.7:
                result["ocena_balansu"] = "zrÃ³wnowaÅ¼ona"
            elif contra_to_pro_ratio > 0.7:
                result["ocena_balansu"] = "silnie dialektyczna"
            else:
                result["ocena_balansu"] = "jednostronna"
        else:
            result["balans_argumentacji"] = 0.0
            result["ocena_balansu"] = "jednokierunkowa"
            
        return result
        
    def analyze_semantic_structure(self, text):
        """Analizuje gÅ‚Ä™bokÄ… strukturÄ™ semantycznÄ… tekstu"""
        if not text:
            return {"struktura": "brak tekstu", "relacje": [], "tematy": []}
            
        text_lower = text.lower()
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        if not sentences:
            return {"struktura": "brak tekstu", "relacje": [], "tematy": []}
            
        # 1. Analiza podmiotÃ³w i obiektÃ³w w tekÅ›cie
        entities = []
        patterns = {
            "osoba": [
                r"\b([A-Z][a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]+)\b(?=\s+(?:powiedziaÅ‚|stwierdziÅ‚|uwaÅ¼a))",
                r"\b([A-Z][a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]+)\b(?=\s+(?:jest|byÅ‚|bÄ™dzie))"
            ],
            "organizacja": [
                r"\b([A-Z][A-ZÄ„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]+)\b(?=\s+(?:ogÅ‚osiÅ‚|poinformowaÅ‚))",
                r"\b(?:firma|spÃ³Å‚ka|organizacja|instytucja|ministerstwo)\s+([A-Z]\w+)\b"
            ],
            "miejsce": [
                r"\bw\s+([A-Z][a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]+)\b",
                r"\bdo\s+([A-Z][a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]+)\b",
                r"\bz\s+([A-Z][a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]+)\b"
            ],
            "czas": [
                r"\b(\d{1,2}\s+(?:stycznia|lutego|marca|kwietnia|maja|czerwca|lipca|sierpnia|wrzeÅ›nia|paÅºdziernika|listopada|grudnia)(?:\s+\d{4})?)\b",
                r"\b((?:w\s+)?(?:poniedziaÅ‚ek|wtorek|Å›rodÄ™|czwartek|piÄ…tek|sobotÄ™|niedzielÄ™))\b",
                r"\b(\d{1,2}:\d{2})\b"
            ],
            "pojÄ™cie": [
                r"\b([a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]{5,}(?:acja|izm|oÅ›Ä‡|stwo|ctwo|anie|enie))\b"
            ]
        }
        
        for entity_type, patterns_list in patterns.items():
            for pattern in patterns_list:
                for sentence in sentences:
                    matches = re.findall(pattern, sentence, re.IGNORECASE)
                    for match in matches:
                        if isinstance(match, tuple):
                            match = match[0]
                        if len(match) > 2:  # Minimalna dÅ‚ugoÅ›Ä‡ encji
                            entities.append({
                                "tekst": match,
                                "typ": entity_type,
                                "kontekst": sentence[:50] + "..." if len(sentence) > 50 else sentence
                            })
        
        # Filtrowanie duplikatÃ³w
        unique_entities = []
        seen_entities = set()
        for entity in entities:
            key = (entity["tekst"].lower(), entity["typ"])
            if key not in seen_entities:
                seen_entities.add(key)
                unique_entities.append(entity)
        
        # 2. Analiza relacji semantycznych
        relations = []
        semantic_patterns = {
            "przyczynowo-skutkowe": [
                r"(\b\w+[^.!?]*)\s+(?:powoduje|powodujÄ…|spowodowaÅ‚|spowodowaÅ‚a|wywoÅ‚uje|skutkuje)\s+([^.!?]*)",
                r"(\b\w+[^.!?]*)\s+(?:wpÅ‚ywa|wpÅ‚ywajÄ…|wpÅ‚ynÄ…Å‚|wpÅ‚ynÄ™Å‚a)\s+na\s+([^.!?]*)",
                r"(\b\w+[^.!?]*)\s+(?:dlatego|z\s+tego\s+powodu|w\s+rezultacie)\s+([^.!?]*)"
            ],
            "porÃ³wnawcze": [
                r"(\b\w+[^.!?]*)\s+(?:podobnie\s+jak|tak\s+jak|podobnie\s+do)\s+([^.!?]*)",
                r"(\b\w+[^.!?]*)\s+(?:rÃ³Å¼ni\s+siÄ™\s+od|jest\s+inne\s+niÅ¼|jest\s+odmienne\s+od)\s+([^.!?]*)",
                r"(\b\w+[^.!?]*)\s+(?:w\s+przeciwieÅ„stwie\s+do)\s+([^.!?]*)"
            ],
            "czÄ™Å›Ä‡-caÅ‚oÅ›Ä‡": [
                r"(\b\w+[^.!?]*)\s+(?:skÅ‚ada\s+siÄ™\s+z|zawiera|obejmuje)\s+([^.!?]*)",
                r"(\b\w+[^.!?]*)\s+(?:jest\s+czÄ™Å›ciÄ…|wchodzi\s+w\s+skÅ‚ad|naleÅ¼y\s+do)\s+([^.!?]*)"
            ],
            "posesywne": [
                r"(\b\w+[^.!?]*)\s+(?:posiada|ma|dysponuje)\s+([^.!?]*)",
                r"(\b\w+[^.!?]*)\s+(?:naleÅ¼Ä…cy\s+do|wÅ‚aÅ›ciciel|wÅ‚asnoÅ›Ä‡)\s+([^.!?]*)"
            ],
            "temporalne": [
                r"(\b\w+[^.!?]*)\s+(?:przed|po|w\s+trakcie|podczas)\s+([^.!?]*)",
                r"(\b\w+[^.!?]*)\s+(?:wczeÅ›niej\s+niÅ¼|pÃ³Åºniej\s+niÅ¼|rÃ³wnoczeÅ›nie\s+z)\s+([^.!?]*)"
            ]
        }
        
        for rel_type, patterns_list in semantic_patterns.items():
            for pattern in patterns_list:
                for sentence in sentences:
                    matches = re.findall(pattern, sentence, re.IGNORECASE)
                    for match in matches:
                        if len(match) >= 2:
                            relations.append({
                                "typ": rel_type,
                                "element_1": match[0].strip(),
                                "element_2": match[1].strip(),
                                "zdanie": sentence
                            })
        
        # 3. Analiza struktury tematycznej
        topic_words = {}
        
        # Ekstrakcja rzeczownikÃ³w jako potencjalnych tematÃ³w
        noun_pattern = r"\b([a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]{3,}(?:oÅ›Ä‡|anie|enie|stwo|ctwo|cja|zja|acja|izm|or|er|yk))\b"
        for sentence in sentences:
            matches = re.findall(noun_pattern, sentence.lower())
            for match in matches:
                if match not in topic_words:
                    topic_words[match] = 0
                topic_words[match] += 1
        
        # Lista "stop words" dla tematÃ³w
        stop_words = ["dlatego", "poniewaÅ¼", "przez", "gdyÅ¼", "czyli", "wiÄ™c", "jednak", 
                     "bowiem", "takÅ¼e", "rÃ³wnieÅ¼", "czyli", "wÅ‚aÅ›nie", "natomiast"]
        
        # Filtrowanie potencjalnych tematÃ³w
        for word in stop_words:
            if word in topic_words:
                del topic_words[word]
        
        # WybÃ³r gÅ‚Ã³wnych tematÃ³w
        main_topics = sorted(topic_words.items(), key=lambda x: x[1], reverse=True)[:5]
        main_topics = [{"temat": topic, "czÄ™stoÅ›Ä‡": count} for topic, count in main_topics]
        
        # 4. Analiza kohezji tekstu (powiÄ…zaÅ„ wewnÄ™trznych)
        cohesion_markers = {
            "zaimki_anaforyczne": [
                r"\b(?:on[a|i]?|jeg[a|o]|jej|ich|t[en|Ä…|ym|ymi]|t[a|e]|ci|tamci|tamte)\b"
            ],
            "odniesienia_tematyczne": [
                r"\b(?:ten\s+sam|wspomnian[y|a|e]|powyÅ¼sz[y|a|e]|wczeÅ›niejsz[y|a|e])\b"
            ],
            "spÃ³jniki_kontynuacji": [
                r"\b(?:ponadto|poza\s+tym|co\s+wiÄ™cej|nastÄ™pnie|dalej|kontynuujÄ…c)\b"
            ],
            "powtÃ³rzenia_leksykalne": []  # BÄ™dzie analizowane algorytmicznie
        }
        
        # Liczenie markerÃ³w kohezji
        cohesion_counts = {}
        for marker_type, patterns_list in cohesion_markers.items():
            cohesion_counts[marker_type] = 0
            for pattern in patterns_list:
                for sentence in sentences:
                    matches = re.findall(pattern, sentence, re.IGNORECASE)
                    cohesion_counts[marker_type] += len(matches)
        
        # Analiza powtÃ³rzeÅ„ leksykalnych
        words_by_sentence = [re.findall(r'\b\w{3,}\b', s.lower()) for s in sentences]
        repetitions = 0
        
        # Szukamy powtÃ³rzeÅ„ sÅ‚Ã³w miÄ™dzy zdaniami
        if len(words_by_sentence) > 1:
            for i in range(1, len(words_by_sentence)):
                for word in words_by_sentence[i]:
                    if word in words_by_sentence[i-1]:
                        repetitions += 1
        
        cohesion_counts["powtÃ³rzenia_leksykalne"] = repetitions
        
        # OgÃ³lna miara kohezji
        cohesion_total = sum(cohesion_counts.values())
        cohesion_per_sentence = cohesion_total / len(sentences) if sentences else 0
        
        # OkreÅ›lenie spÃ³jnoÅ›ci semantycznej
        cohesion_level = "niska"
        if cohesion_per_sentence >= 1.5:
            cohesion_level = "wysoka"
        elif cohesion_per_sentence >= 0.8:
            cohesion_level = "Å›rednia"
        
        # 5. OkreÅ›lenie gÅ‚Ã³wnej struktury semantycznej
        semantic_structure_types = {
            "narracyjna": 0,
            "ekspozycyjna": 0, 
            "argumentacyjna": 0,
            "opisowa": 0,
            "instruktaÅ¼owa": 0
        }
        
        # Wzorce jÄ™zykowe charakterystyczne dla poszczegÃ³lnych struktur
        structure_patterns = {
            "narracyjna": [
                r"\b(?:najpierw|potem|nastÄ™pnie|wtedy|pÃ³Åºniej|w\s+koÅ„cu)\b",
                r"\b(?:gdy|kiedy|podczas|po\s+tym\s+jak|zanim|wkrÃ³tce)\b",
                r"\b(?:pewnego\s+dnia|pewnego\s+razu|dawno\s+temu|kiedyÅ›)\b"
            ],
            "ekspozycyjna": [
                r"\b(?:definiuje|klasyfikuje|wyjaÅ›nia|przedstawia|omawia)\b",
                r"\b(?:po\s+pierwsze|po\s+drugie|jednym\s+z|kolejnym)\b",
                r"\b(?:gÅ‚Ã³wnym|kluczowym|istotnym|waÅ¼nym|podstawowym)\b"
            ],
            "argumentacyjna": [
                r"\b(?:twierdzÄ™|uwaÅ¼am|sÄ…dzÄ™|dowodzÄ™|argumentujÄ™|przekonujÄ™)\b",
                r"\b(?:poniewaÅ¼|dlatego|zatem|wobec\s+tego|wynika\s+z\s+tego)\b",
                r"\b(?:podsumowujÄ…c|w\s+konkluzji|z\s+tego\s+wynika|dowodzi\s+to)\b"
            ],
            "opisowa": [
                r"\b(?:wyglÄ…da\s+jak|przypomina|charakteryzuje\s+siÄ™|cechuje\s+siÄ™)\b",
                r"\b(?:jest|wydaje\s+siÄ™|sprawia\s+wraÅ¼enie|prezentuje\s+siÄ™\s+jako)\b",
                r"\b(?:czerwony|niebieski|zielony|duÅ¼y|maÅ‚y|szeroki|wÄ…ski|wysoki)\b"
            ],
            "instruktaÅ¼owa": [
                r"\b(?:naleÅ¼y|trzeba|powinno\s+siÄ™|musisz|najpierw|nastÄ™pnie)\b",
                r"\b(?:krok\s+po\s+kroku|w\s+pierwszej\s+kolejnoÅ›ci|na\s+koÅ„cu)\b",
                r"(?:^\s*\d+\.|\d\)\s+|\-\s+|â€¢\s+)"
            ]
        }
        
        # Analiza wzorcÃ³w dla okreÅ›lenia struktury
        for structure, patterns_list in structure_patterns.items():
            for pattern in patterns_list:
                for sentence in sentences:
                    matches = re.findall(pattern, sentence, re.IGNORECASE)
                    semantic_structure_types[structure] += len(matches)
        
        main_structure = max(semantic_structure_types.items(), key=lambda x: x[1])
        main_structure_type = main_structure[0]
        if main_structure[1] == 0:
            main_structure_type = "mieszana/nieokreÅ›lona"
        
        result = {
            "struktura_semantyczna": main_structure_type,
            "encje": unique_entities[:10],  # Ograniczamy do top 10
            "relacje": relations[:10],      # Ograniczamy do top 10
            "gÅ‚Ã³wne_tematy": main_topics,
            "spÃ³jnoÅ›Ä‡": {
                "poziom": cohesion_level,
                "markery_kohezji": cohesion_counts,
                "wskaÅºnik_spÃ³jnoÅ›ci": round(cohesion_per_sentence, 2)
            }
        }
        
        return result
    
    def detect_temporal_context(self, text):
        """Wykrywanie kontekstu czasowego w tekÅ›cie"""
        text_lower = text.lower()
        temporal_scores = {"przeszÅ‚oÅ›Ä‡": 0, "teraÅºniejszoÅ›Ä‡": 0.1, "przyszÅ‚oÅ›Ä‡": 0}
        
        # WskaÅºniki czasu przeszÅ‚ego
        past_indicators = ["byÅ‚", "byÅ‚a", "byÅ‚o", "byÅ‚y", "byÅ‚em", "byÅ‚am", "zrobiÅ‚em", "zrobiÅ‚am", "wczoraj", "wczeÅ›niej", "dawniej", "kiedyÅ›", "niedawno"]
        
        # WskaÅºniki czasu teraÅºniejszego
        present_indicators = ["jest", "sÄ…", "jestem", "jesteÅ›", "robimy", "robiÄ™", "teraz", "obecnie", "dziÅ›", "dzisiaj"]
        
        # WskaÅºniki czasu przyszÅ‚ego
        future_indicators = ["bÄ™dzie", "bÄ™dÄ…", "bÄ™dÄ™", "bÄ™dziemy", "zrobimy", "zrobiÄ™", "jutro", "wkrÃ³tce", "za chwilÄ™", "w przyszÅ‚oÅ›ci", "pÃ³Åºniej"]
        
        # Sprawdzanie wskaÅºnikÃ³w w tekÅ›cie
        for indicator in past_indicators:
            if indicator in text_lower:
                temporal_scores["przeszÅ‚oÅ›Ä‡"] += 0.15
                
        for indicator in present_indicators:
            if indicator in text_lower:
                temporal_scores["teraÅºniejszoÅ›Ä‡"] += 0.15
                
        for indicator in future_indicators:
            if indicator in text_lower:
                temporal_scores["przyszÅ‚oÅ›Ä‡"] += 0.15
        
        # Normalizacja wynikÃ³w
        total = sum(temporal_scores.values()) or 1
        normalized = {k: round(v/total, 2) for k, v in temporal_scores.items()}
        
        # OkreÅ›lenie dominujÄ…cego kontekstu czasowego
        dominant = max(normalized, key=normalized.get)
        normalized["dominujÄ…cy"] = dominant
        
        return normalized
    def extract_entities(self, text):
        """Ekstrakcja encji z tekstu (osoby, miejsca, organizacje, daty, liczby)"""
        entities = {
            "osoby": [],
            "miejsca": [],
            "organizacje": [],
            "daty": [],
            "liczby": []
        }
        
        # Proste wzorce do rozpoznawania encji
        
        # Osoby (podstawowy wzorzec imiÄ™ i nazwisko)
        person_pattern = re.compile(r'\b[A-ZÅšÄ†Å¹Å»ÅÃ“Åƒ][a-zÅ›Ä‡ÅºÅ¼Å‚Ã³Å„Ã¤Ã«Ã¶Ã¼ÃŸ]+ [A-ZÅšÄ†Å¹Å»ÅÃ“Åƒ][a-zÅ›Ä‡ÅºÅ¼Å‚Ã³Å„Ã¤Ã«Ã¶Ã¼ÃŸ]+\b')
        for match in person_pattern.finditer(text):
            entities["osoby"].append(match.group(0))
            
        # Miejsca (miasta, kraje)
        places = ["Warszawa", "KrakÃ³w", "WrocÅ‚aw", "PoznaÅ„", "GdaÅ„sk", "ÅÃ³dÅº", "Szczecin", 
                 "Polska", "Niemcy", "Francja", "WÅ‚ochy", "Hiszpania", "Anglia", "USA"]
        for place in places:
            if place in text:
                entities["miejsca"].append(place)
                
        # Organizacje (proste wzorce)
        org_pattern = re.compile(r'\b(?:[A-ZÅšÄ†Å¹Å»ÅÃ“Åƒ][a-zÅ›Ä‡ÅºÅ¼Å‚Ã³Å„Ã¤Ã«Ã¶Ã¼ÃŸ]+ )?(?:[A-ZÅšÄ†Å¹Å»ÅÃ“Åƒ][a-zÅ›Ä‡ÅºÅ¼Å‚Ã³Å„Ã¤Ã«Ã¶Ã¼ÃŸ]+ )?[A-ZÅšÄ†Å¹Å»ÅÃ“Åƒ][a-zÅ›Ä‡ÅºÅ¼Å‚Ã³Å„Ã¤Ã«Ã¶Ã¼ÃŸ]* (?:sp\. z o\.o\.|S\.A\.|Inc\.|Ltd\.|GmbH)\b')
        for match in org_pattern.finditer(text):
            entities["organizacje"].append(match.group(0))
            
        # Popularne organizacje
        orgs = ["Google", "Microsoft", "Facebook", "Apple", "Amazon", "Twitter", "Netflix", "Allegro", "PKO", "PZU"]
        for org in orgs:
            if org in text:
                entities["organizacje"].append(org)
                
        # Daty
        date_patterns = [
            re.compile(r'\b\d{1,2} (?:stycznia|lutego|marca|kwietnia|maja|czerwca|lipca|sierpnia|wrzeÅ›nia|paÅºdziernika|listopada|grudnia) \d{4}\b'),
            re.compile(r'\b\d{1,2}\.\d{1,2}\.\d{2,4}\b'),
            re.compile(r'\b\d{4}-\d{1,2}-\d{1,2}\b')
        ]
        
        for pattern in date_patterns:
            for match in pattern.finditer(text):
                entities["daty"].append(match.group(0))
                
        # Liczby
        number_pattern = re.compile(r'\b\d+(?:[.,]\d+)?\b')
        for match in number_pattern.finditer(text):
            entities["liczby"].append(match.group(0))
            
        # UsuniÄ™cie duplikatÃ³w
        for entity_type in entities:
            entities[entity_type] = list(set(entities[entity_type]))
            
        return entities
    
    def analyze_conversation(self, messages):
        """Analiza caÅ‚ej konwersacji"""
        if not messages:
            return {}
            
        # Ekstrahuj teksty z wiadomoÅ›ci
        texts = [msg.get("content", "") for msg in messages]
        full_text = " ".join(texts)
        
        # Analiza caÅ‚ego tekstu
        overall_analysis = self.analyze_text(full_text)
        
        # Åšledzenie trendu sentymentu
        sentiment_values = []
        for msg in messages:
            content = msg.get("content", "")
            if content:
                sentiment = self.analyze_sentiment(content)
                if sentiment["dominujÄ…cy"] == "pozytywny":
                    sentiment_values.append(sentiment["pozytywny"])
                elif sentiment["dominujÄ…cy"] == "negatywny":
                    sentiment_values.append(-sentiment["negatywny"])
                else:
                    sentiment_values.append(0)
        
        # OkreÅ›lenie trendu sentymentu
        sentiment_trend = "stabilny"
        avg_sentiment = "neutralny"
        if sentiment_values:
            if len(sentiment_values) >= 3:
                first_half = sentiment_values[:len(sentiment_values)//2]
                second_half = sentiment_values[len(sentiment_values)//2:]
                avg_first = sum(first_half) / len(first_half) if first_half else 0
                avg_second = sum(second_half) / len(second_half) if second_half else 0
                
                if avg_second > avg_first + 0.2:
                    sentiment_trend = "rosnÄ…cy"
                elif avg_second < avg_first - 0.2:
                    sentiment_trend = "malejÄ…cy"
            
            avg_value = sum(sentiment_values) / len(sentiment_values)
            if avg_value > 0.2:
                avg_sentiment = "pozytywny"
            elif avg_value < -0.2:
                avg_sentiment = "negatywny"
        
        # Analiza spÃ³jnoÅ›ci tematycznej
        topic_consistency = {"spÃ³jnoÅ›Ä‡": "wysoka", "wartoÅ›Ä‡": 0.8}
        if len(texts) >= 2:
            topics_per_message = [set(self.detect_topics(txt).keys()) for txt in texts]
            consistency_scores = []
            
            for i in range(1, len(topics_per_message)):
                current = topics_per_message[i]
                previous = topics_per_message[i-1]
                
                if current and previous:  # JeÅ›li oba zestawy niepuste
                    similarity = len(current.intersection(previous)) / len(current.union(previous)) if current.union(previous) else 0
                    consistency_scores.append(similarity)
            
            if consistency_scores:
                avg_consistency = sum(consistency_scores) / len(consistency_scores)
                topic_consistency["wartoÅ›Ä‡"] = round(avg_consistency, 2)
                
                if avg_consistency < 0.3:
                    topic_consistency["spÃ³jnoÅ›Ä‡"] = "niska"
                elif avg_consistency < 0.6:
                    topic_consistency["spÃ³jnoÅ›Ä‡"] = "Å›rednia"
        
        # Analiza zmian intencji
        intention_sequence = []
        for msg in messages:
            if msg.get("role") == "user" and msg.get("content"):
                intention = self.detect_intention(msg.get("content"))
                intention_sequence.append(intention["dominujÄ…ca"])
                
        intention_changes = {"zmiany": "brak", "sekwencja": intention_sequence}
        
        if len(intention_sequence) >= 3:
            changes_count = sum(1 for i in range(1, len(intention_sequence)) if intention_sequence[i] != intention_sequence[i-1])
            change_rate = changes_count / (len(intention_sequence) - 1)
            
            if change_rate > 0.7:
                intention_changes["zmiany"] = "czÄ™ste"
            elif change_rate > 0.3:
                intention_changes["zmiany"] = "sporadyczne"
        
        return {
            "overall_analysis": overall_analysis,
            "sentiment_trend": {
                "trend": sentiment_trend,
                "Å›redni_sentyment": avg_sentiment,
                "wartoÅ›ci": sentiment_values
            },
            "topic_consistency": topic_consistency,
            "main_topics": list(overall_analysis["topics"].keys()),
            "intention_changes": intention_changes
        }

class SemanticIntegration:
    """Klasa integrujÄ…ca analizÄ™ semantycznÄ… z gÅ‚Ã³wnym systemem"""
    def __init__(self, db_path=None):
        self.db_path = db_path
        self.analyzer = SemanticAnalyzer()
        
        # Inicjalizacja tabeli semantic_metadata, jeÅ›li nie istnieje
        if db_path:
            conn = sqlite3.connect(db_path)
            conn.execute('''
                CREATE TABLE IF NOT EXISTS semantic_metadata (
                    id TEXT PRIMARY KEY,
                    user_id TEXT,
                    message_id TEXT,
                    role TEXT,
                    topics TEXT,
                    sentiment TEXT,
                    intention TEXT,
                    entities TEXT,
                    complexity TEXT,
                    temporal_context TEXT,
                    timestamp REAL
                )
            ''')
            conn.commit()
            conn.close()
    
    def enhance_chat_response(self, user_id, query, response, message_history=None):
        """Wzbogaca odpowiedÅº o analizÄ™ semantycznÄ…"""
        # Analiza zapytania uÅ¼ytkownika
        query_analysis = self.analyzer.analyze_text(query)
        
        # Analiza odpowiedzi
        response_analysis = self.analyzer.analyze_text(response)
        
        # Analiza caÅ‚ej konwersacji
        conversation_analysis = {}
        if message_history:
            conversation_analysis = self.analyzer.analyze_conversation(message_history)
        
        # Ekstrakcja encji
        entities = query_analysis.get("entities", {})
        
        # Generowanie rekomendacji
        recommendations = self._generate_recommendations(query_analysis, response_analysis, conversation_analysis)
        
        # Zapisanie metadanych semantycznych w bazie danych
        self.store_semantic_data(user_id, query, query_analysis)
        
        return {
            "query_analysis": query_analysis,
            "response_analysis": response_analysis,
            "conversation_analysis": conversation_analysis,
            "entities": entities,
            "recommendations": recommendations
        }
    
    def _generate_recommendations(self, query_analysis, response_analysis, conversation_analysis):
        """Generuje rekomendacje na podstawie analizy semantycznej"""
        recommendations = {
            "topics": [],
            "intentions": [],
            "tone_suggestions": [],
            "follow_up_questions": [],
            "context_needs": []
        }
        
        # Rekomendacje tematÃ³w
        if query_analysis.get("topics"):
            for topic, score in sorted(query_analysis["topics"].items(), key=lambda x: x[1], reverse=True)[:3]:
                recommendations["topics"].append({"topic": topic, "score": score})
        
        # Rekomendacje intencji
        query_intention = query_analysis.get("intention", {}).get("dominujÄ…ca")
        if query_intention:
            recommendations["intentions"].append(query_intention)
        
        # Rekomendacje tonu
        query_sentiment = query_analysis.get("sentiment", {}).get("dominujÄ…cy")
        if query_sentiment == "pozytywny":
            recommendations["tone_suggestions"] = ["pozytywny", "entuzjastyczny", "pomocny"]
        elif query_sentiment == "negatywny":
            recommendations["tone_suggestions"] = ["empatyczny", "profesjonalny", "rzeczowy"]
        else:
            recommendations["tone_suggestions"] = ["informacyjny", "neutralny", "rzeczowy"]
        
        # Generowanie pytaÅ„ uzupeÅ‚niajÄ…cych
        topics = list(query_analysis.get("topics", {}).keys())
        entities = query_analysis.get("entities", {})
        keywords = query_analysis.get("keywords", [])
        
        follow_up_templates = [
            "Czy potrzebujesz bardziej szczegÃ³Å‚owych informacji na temat {topic}?",
            "Czy chciaÅ‚byÅ› dowiedzieÄ‡ siÄ™ wiÄ™cej o {keyword}?",
            "Czy masz jakieÅ› konkretne pytania dotyczÄ…ce {entity}?",
            "Czy mogÄ™ pomÃ³c Ci w jeszcze czymÅ› zwiÄ…zanym z {topic}?"
        ]
        
        # Tworzenie pytaÅ„ uzupeÅ‚niajÄ…cych
        if topics:
            topic = random.choice(topics)
            recommendations["follow_up_questions"].append(follow_up_templates[0].format(topic=topic))
            
        if keywords:
            keyword = random.choice(keywords)
            recommendations["follow_up_questions"].append(follow_up_templates[1].format(keyword=keyword))
            
        for entity_type, entity_list in entities.items():
            if entity_list and random.random() < 0.5:  # 50% szans na dodanie pytania o encjÄ™
                entity = random.choice(entity_list)
                recommendations["follow_up_questions"].append(follow_up_templates[2].format(entity=entity))
        
        # Potrzeby kontekstowe
        if not entities.get("osoby") and ("osoba" in topics or "ludzie" in topics):
            recommendations["context_needs"].append("informacje o osobach")
            
        if not entities.get("daty") and ("czas" in topics or "harmonogram" in topics):
            recommendations["context_needs"].append("informacje o czasie/datach")
            
        # UsuniÄ™cie duplikatÃ³w i ograniczenie do sensownej liczby
        recommendations["follow_up_questions"] = list(set(recommendations["follow_up_questions"]))[:3]
        
        return recommendations
    
    def get_semantic_metadata_for_db(self, user_id, text, role):
        """Przygotowuje metadane semantyczne do zapisu w bazie danych"""
        analysis = self.analyzer.analyze_text(text)
        
        semantic_metadata = {
            "user_id": user_id,
            "role": role,
            "semantic_metadata": {
                "topics": analysis.get("topics", {}),
                "sentiment": analysis.get("sentiment", {}).get("dominujÄ…cy", "neutralny"),
                "intention": analysis.get("intention", {}).get("dominujÄ…ca", "nieznana"),
                "entities": analysis.get("entities", {}),
                "complexity": analysis.get("complexity", {}).get("poziom", "Å›rednia"),
                "temporal_context": analysis.get("temporal_context", {}).get("dominujÄ…cy", "teraÅºniejszoÅ›Ä‡")
            }
        }
        
        return semantic_metadata
        
    def store_semantic_data(self, user_id, query, analysis):
        """Zapisuje metadane semantyczne w bazie danych"""
        if not self.db_path or not user_id or not query:
            return False
            
        try:
            conn = sqlite3.connect(self.db_path)
            c = conn.cursor()
            id = uuid.uuid4().hex
            message_id = uuid.uuid4().hex
            
            # Konwersja pÃ³l sÅ‚ownikowych do JSON
            topics_json = json.dumps(analysis.get("topics", {}), ensure_ascii=False)
            sentiment = analysis.get("sentiment", {}).get("dominujÄ…cy", "neutralny")
            intention = analysis.get("intention", {}).get("dominujÄ…ca", "nieznana")
            entities_json = json.dumps(analysis.get("entities", {}), ensure_ascii=False)
            complexity = analysis.get("complexity", {}).get("poziom", "Å›rednia")
            temporal_context = analysis.get("temporal_context", {}).get("dominujÄ…cy", "teraÅºniejszoÅ›Ä‡")
            
            c.execute("INSERT INTO semantic_metadata VALUES(?,?,?,?,?,?,?,?,?,?,?)",
                     (id, user_id, message_id, "user", topics_json, sentiment, intention, entities_json, complexity, temporal_context, time.time()))
            
            conn.commit()
            conn.close()
            return True
        except Exception as e:
            print(f"BÅ‚Ä…d podczas zapisywania danych semantycznych: {e}")
            return False

# Inicjalizacja moduÅ‚u analizy semantycznej
semantic_analyzer = SemanticAnalyzer()

# Inicjalizacja moduÅ‚u analizy semantycznej
semantic_analyzer = SemanticAnalyzer()
semantic_integration = SemanticIntegration(DB_PATH)
log_info("Semantic module initialized", "SEMANTIC")

# Public API
def semantic_analyze(text: str)->Dict[str,Any]:
    """Analyze text semantically"""
    return semantic_analyzer.analyze_text(text)

def semantic_analyze_conversation(messages: List[Dict[str,str]])->Dict[str,Any]:
    """Analyze entire conversation"""
    return semantic_analyzer.analyze_conversation(messages)

def semantic_enhance_response(answer: str, context: str="")->Dict[str,Any]:
    """Enhance response based on semantic analysis"""
    analysis = semantic_analyzer.analyze_text(answer)
    enhanced = answer
    sentiment = analysis.get("sentyment", {})
    
    if sentiment.get("dominujÄ…cy") == "negatywny":
        if not any(word in answer.lower() for word in ["przepraszam", "rozumiem", "przykro"]):
            enhanced = "Rozumiem, " + answer[0].lower() + answer[1:]
    
    return {"ok": True, "original": answer, "enhanced": enhanced, "analysis": analysis}


def embed_text(text: str) -> List[float]:
    """
    Generuje embedding dla tekstu uÅ¼ywajÄ…c dostÄ™pnego providera.
    
    Args:
        text: Tekst do embeddingu
        
    Returns:
        List[float]: Wektor embedding
    """
    try:
        # UÅ¼yj semantic_analyze do uzyskania embedding
        analysis = semantic_analyze(text)
        return analysis.get("embedding", [])
    except Exception as e:
        log_error(f"Text embedding failed: {e}")
        return []


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """
    Oblicza podobieÅ„stwo kosinusowe miÄ™dzy dwoma wektorami.
    
    Args:
        vec1: Pierwszy wektor
        vec2: Drugi wektor
        
    Returns:
        float: PodobieÅ„sto w zakresie [0, 1]
    """
    try:
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0
            
        import math
        
        # Oblicz iloczyn skalarny
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        
        # Oblicz normy
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        return dot_product / (norm1 * norm2)
        
    except Exception as e:
        log_error(f"Cosine similarity calculation failed: {e}")
        return 0.0


__all__ = [
    'SemanticAnalyzer', 'SemanticIntegration',
    'semantic_analyzer', 'semantic_integration',
    'semantic_analyze', 'semantic_analyze_conversation', 'semantic_enhance_response',
    'embed_text', 'cosine_similarity'
]
